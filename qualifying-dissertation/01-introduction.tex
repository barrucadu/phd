\part{Introduction}

% "identify and describe in outline your chosen field or research;
% explain the motivation for research in that field."

\chapter{Introduction}
\label{chp:intro}

My research is concerned with the problems of nondeterministic
concurrency in pure functional programming languages, and how to wield
this powerful tool whilst avoiding possibly subtle programming
errors. My goal is to implement libraries and tools to enable
programmers to make use of concurrency whilst being confident of the
correctness of their programs. The questions I want to answer are:

\begin{itemize}
  \item Can existing results from concurrency testing in other
    languages be reapplied to the pure functional setting?

  \item Are pure functional languages a good candidate for the static,
    automated, analysis or verification of concurrency correctness
    properties?

  \item \textbf{Can it be made much easier to write correct concurrent
      code?}
\end{itemize}

Concurrency is notoriously difficult to get right\cite{overrated},
sometimes even leading to death\cite{therac25}. The problem largely
stems from the nondeterminism of scheduling: the same program with the
same inputs may produce different results depending on the schedules
chosen at runtime. This makes it difficult to use traditional testing
techniques with concurrent programs, which rely on the result of
executing a test to be deterministic. So-called ``Heisenbugs'' make it
difficult to be confident of the correctness of concurrent programs:
no bug has been observed during the testing process, but how do we
\textit{know} that there aren't any?

Despite the difficulty, concurrency is important for producing many
real-world applications. Frameworks for graphical user interfaces
typically work by executing the code which governs the GUI in a
separate thread to the application logic, with communication between
the two. Applications with a lot of input and output can be more
responsive to local actions by executing I/O asynchronously, in
separate threads. Concurrency is a useful program structuring
technique, and it is here to stay.

There are now a few well-known techniques to avoid concurrency bugs,
such as protecting mutable state with locks, and acquiring locks in a
fixed order. Exercises like the Dining
Philosophers\cite{diningphilosophers} and the Santa Claus
Problem\cite{santaclaus} allow programmers to explore these topics in
small well-understood settings. However, as systems grow, it becomes
difficult to think about how different components interact, and it is
easy to slip up and introduce a bug.

\section{Parallelism vs. Concurrency}
\label{sec:intro-parconc}

It is worth clarifying at this early stage some terminology which will
be frequently used throughout this report:

\begin{description}
  \item[Concurrency] is a programming methodology, using concepts such
    as threads, locks, and mutable variables to structure
    programs.

  \item[Parallelism] is an implementation detail, where a multiplicity
    of hardware components are used to execute distinct pieces of code
    simultaneously.
\end{description}

Concurrency does not require parallelism, as demonstrated by the
single-core, single-processor computers of yore. Similarly,
parallelism does not require concurrency, as demonstrated by the
data-parallel x86 assembly instructions such as \verb|PMULHUW|, which
computes an element-wise multiplication of two vectors, each
multiplication in parallel.

Unrestricted concurrency is explicit and semantically
visible\cite{concurrent}. The interleaved execution of threads, when
combined with mutable state, gives rise to nondeterminism. Semaphores
and locks give rise to termination errors in the form of deadlock and
livelock. Parallelism, in particular the parallel evaluation of
expressions, is semantically invisible in a language without
side-effects, and there is the possibility for implicit parallelism:
``at present these combinators [\verb|par| and \verb|seq|] are added
by the programmer, though we would of course like this task to be
automated.''\cite{gum}

Concurrency is often implemented using parallelism, and indeed a
concurrency abstraction can be used to guarantee parallelism (given
suitable hardware), for example by having the ability to restrict the
execution of individual threads to given processors.

This report is largely concerned with concurrency, although
parallelism is also discussed as it is often the motivation to use
concurrency.

\section{Testing Concurrent Programs}
\label{sec:intro-sct}

The problem of testing concurrent programs is twofold: the scheduler
is a part of the language runtime system (or the operating system),
and so out of the reach of the programmer and tester; and also
scheduling decisions are nondeterministic. Given these, how can we
write reproducible tests?

\textit{Systematic concurrency testing} (SCT)\cite{pbound, dbound,
  empirical, heisenbugs} is a technique for avoiding the problem of
nondeterminism when writing tests. It aims to test a large number of
schedules, whilst typically also making use of local knowledge of the
program to reduce the number of schedules needed to be confident of an
accurate result. By testing many schedules, we can be confident that
any bugs which have not been found are unlikely to be exhibited.

SCT overcomes the scheduling problem by forcing a concurrent program
to use instead a scheduler implemented as part of the testing
framework: either by overriding the concurrency primitives of the
language (as in LazyLocks\footnote{(Paul Thompson) to appear}), or by
modifying the program under test to call out to this new scheduler (as
in PULSE\cite{pulse}).

Once the scheduler is under control, schedules can be recorded and
replayed, giving reproducibility. Furthermore, by observing which
scheduling decisions are available at each decision point, possible
schedules can be systematically explored, making different decisions
on subsequent executions. Common methods of choosing schedules to take
are random\cite{empirical}, pre-emption bounding\cite{pbound}, and
delay bounding\cite{dbound}.

\begin{description}
  \item[Random] scheduling is just that, at each decision point a
    random decision is made. No guarantees about completeness or lack
    of wasted work are made.

  \item[Pre-emption bounding] explores all schedules with $n$
    pre-emptions, where a pre-emption is a context switch (a change
    from one thread to another) where the original thread was not
    blocked. Typically testing is repeated with increasing values of
    $n$, to explore all schedules with up to some number of
    pre-emptions.

  \item[Delay bounding] explores all schedules with $n$ or fewer
    deviations from an otherwise deterministic scheduler.
\end{description}

Iterative pre-emption bounding gives a global property of schedules,
that there are no errors that can be exhibited with $n$ or fewer
pre-emptions, whereas delay bounding gives a result conditional on the
initial choice of deterministic scheduler. Despite this, they both
perform about as well as each other in terms of bug finding
ability\cite{empirical}. The number of schedules explored by delay
bounding grows more slowly than with pre-emption
bounding\cite{dbound}, perhaps making it a more attractive choice
during development, with pre-emption bounding reserved for producing
more concrete guarantees ahead of software releases. Random
scheduling, surprisingly, performs about as well as schedule bounding
approaches on a standard collection of SCT benchmarks\cite{empirical}.

SCT is a powerful testing technique for concurrent programs, however
it seems to have received little attention in the world of functional
programming. This is possibly because of the large emphasis on
deterministic parallelism. However, non-deterministic concurrency is
still used by functional programmers (often in the implementation of
deterministic abstractions!), and so there is possibly a place for SCT
in this world.

\section{Structure of this Report}
\label{sec:intro-outline}

\begin{itemize}
  \item Chapter \ref{chp:litrev} surveys programming technologies for
    nondeterministic concurrency in pure functional programming
    languages. It focuses on libraries for deterministic parallelism
    built atop concurrency, and on testing tools.

  \item Chapter \ref{chp:dejafu} contains a paper submitted to the
    2015 Haskell Symposium\cite{dejafu} on implementing and using
    systematic concurrency testing in Haskell. It presents a
    generalisation of the standard Haskell concurrency API allowing
    testing, and presents a number of examples, including two from
    pre-existing codebases.

  \item Chapter \ref{chp:searchparty} contains a paper submitted to
    the 2015 Haskell Symposium\cite{searchparty} on parallelising
    generate-and-test computations by making use of nondeterministic
    concurrency. The chapter uses the standard concurrency API for
    ease of presentation, but the real implementation uses \dejafu{}
    for testing purposes.

  \item Chapter \ref{chp:proposal} outlines a proposal for a research
    programme. Both short- and long-term goals are identified and a
    strategy for reaching them is discussed.
\end{itemize}
