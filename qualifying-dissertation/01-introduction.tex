\part{Introduction}

% "identify and describe in outline your chosen field or research;
% explain the motivation for research in that field."

\chapter{Introduction}
\label{chp:intro}

My research is concerned with the problems of nondeterministic
concurrency in lazy pure functional programming languages, and how to
wield this powerful tool whilst avoiding possibly subtle programming
errors.

Concurrency is notoriously difficult to get right\cite{overrated},
sometimes even leading to death\cite{therac25}. The problem largely
stems from the nondeterminism of scheduling, the same program with the
same inputs may produce different results depending on the schedules
chosen at runtime. This makes it difficult to use traditional testing
techniques with concurrent programs, and to spot potentially flawed
code when reading it. These ``Heisenbugs'' make it difficult to be
confident of the correctness of concurrent programs: no bug has been
observed during the testing process, but how do we \textit{know} that
there aren't any?

There are now a few well-known techniques to avoid concurrency bugs,
such as protecting mutable state with locks, and acquiring locks in a
fixed order. Exercises like the Dining
Philosophers\cite{diningphilosophers} and the Santa Claus
Problem\cite{santaclaus} allow programmers to explore these topics in
small well-understood settings. However, as systems grow, it becomes
difficult to think about how different components interact, and it is
easy to slip up and introduce a bug.

\textit{Systematic concurrency testing} (SCT)\cite{pbound, dbound,
  empirical, heisenbugs} is a technique for avoiding the problem of
nondeterminism when writing tests. It aims to test a large number of
schedules, whilst typically also making use of local knowledge of the
program to reduce the number of schedules needed to be confident of an
accurate result. By testing many schedules, we can be confident that
any bugs which have not been found are unlikely to be exhibited.

\section{Parallelism vs. Concurrency}
\label{sec:intro-parconc}

It is worth clarifying at this early stage some terminology which will
be frequently used throughout this report:

\begin{description}
  \item[Concurrency] is a programming methodology using concepts such
    as threads, locks, and mutable variables to structure
    programs.

  \item[Parallelism] is an implementation detail which does not change
    the semantics of the program.
\end{description}

Given these definitions, we might well ask: whence the nondeterminism?
Furthermore, it seems quite an audacious claim that parallelism does
not alter the semantics of the program, but this comes down to how
pure functional languages work. In such a language, evaluation and
execution are separate concerns, we can imagine the overall execution
of a program as evaluating some IO-performing subcomputation, which is
then executed, which may in turn require other things to be
evaluated. Evaluation itself has no side-effects, only execution does,
and so parallel evaluation cannot give a different result than
sequential evaluation. Concurrency, on the other hand, has
nondeterministic scheduling during execution as a part of its
semantics\cite{parconc}, which is where the problem comes from.

Naturally, concurrency often makes use of parallelism, however
concurrency could just as easily be implemented with time-slicing,
across one or possibly many processors. When multiple processors are
introduced, issues such as out-of-order memory behaviour with
IORefs\cite{ioref} becomes possible, but this still does not require
parallelism over concurrency, as the problem could be replicated by
performing some computation on one processor, stopping, and resuming
on another without flushing memory.

\section{Systematic Concurrency Testing}
\label{sec:intro-sct}

The problem of testing concurrent programs is that the scheduler is a
part of the language runtime (or the operating system), and so out of
the reach of the programmer and tester. SCT overcomes this problem by
forcing a concurrent program to instead use a scheduler implemented as
part of the testing framework: either by overriding the concurrency
primitives of the language (as in LazyLocks\footnote{(Paul Thompson) to
  appear}), or by modifying the program under test to call out to this
new scheduler (as in PULSE\cite{pulse}).

Once the scheduler is under control schedules can be recorded and
replayed, giving reproducibility. Furthermore, by observing which
scheduling decisions are available at each decision point, possible
schedules can be systematically explored, making different decisions
on subsequent executions. Common methods of choosing schedules to take
are random, pre-emption bounding\cite{pbound}, and delay
bounding\cite{dbound}.

\begin{description}
  \item[Random] scheduling is just that, at each decision point a
    random decision is made. No guarantees about completeness of lack
    of wasted work are made.

  \item[Pre-emption bounding] explores all schedules with $n$
    pre-emptions, where a pre-emption is a context switch (a change
    from one thread to another) where the original thread was not
    blocked. Typically this is iterated with increasing values of $n$,
    to explore all schedules with up to some number of pre-emptions.

  \item[Delay bounding] explores all schedules with $n$ or fewer
    deviations from an otherwise deterministic scheduler.
\end{description}

Iterative pre-emption bounding gives a global property of schedules,
that there are no errors that can be exhibited with $n$ or fewer
pre-emptions, whereas delay bounding gives a result conditional on the
initial choice of deterministic scheduler. Despite this, they both
perform about as well as each other in terms of bug finding
ability\cite{empirical}, and the number of schedules explored by delay
bounding grows more slowly than with pre-emption
bounding\cite{dbound}, perhaps making it a more attractive choice for
during development, with pre-emption bounding reserved for producing
more concrete guarantees ahead of software releases.

Random scheduling, surprisingly, performs about as well as schedule
bounding approaches on a standard collection of SCT
benchmarks\cite{empirical}.

One concern with SCT is that as concurrent programs grow, the number
of possible interleavings of operations grows much faster, resulting
in an explosion of possible schedules. There is evidence that many
concurrency bugs can be found with small test cases with few threads
and context switches\cite{pbound, dbound, empirical}, providing hope
that unit-test-like test cases can be produced and known-good
components composed efficiently.

Another is that test cases suitable for SCT must be deterministic when
the schedule is fixed, which often isn't easy to achieve if external
processes or network communication is involved. Producing small
suitable tests may be difficult.

\section{Structure of this Report}
\label{sec:intro-outline}

\begin{itemize}
  \item \todo{lit review}
  \item Chapter \ref{chp:dejafu} contains a paper submitted to the
    2015 Haskell Symposium\cite{dejafu} on implementing and using
    systematic concurrency testing in Haskell. It presents a
    generalisation of the standard Haskell concurrency API allowing
    testing, and presents a number of examples, including two from
    pre-existing codebases.

  \item Chapter \ref{chp:searchparty} contains a paper submitted to
    the 2015 Haskell Symposium\cite{searchparty} on parallelising
    generate-and-test computations by making use of deterministic
    concurrency. The chapter uses the standard concurrency API for
    ease of presentation, but the real implementation uses \dejafu{}
    for testing purposes.
  \item Chapter \ref{chp:proposal} outlines a proposal for a research
    programme. Both short- and long-term goals are identified and a
    strategy for reaching them is discussed.
\end{itemize}
