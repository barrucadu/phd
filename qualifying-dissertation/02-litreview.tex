\part{Literature Review}

% "give a thorough account of previous and current work in the field,
% with ample citations of relevant literature; assess the current
% state of the field, for example, discuss assumptions generally made
% and their validity, limitations generally accepted and their
% necessity, major open problems and prospects for their solution and
% the relative strengths and weaknesses of the major lines of work
% pursued to date."

\chapter{Nondeterministic Concurrency in Pure Functional Languages}
\label{chp:litrev}

\textit{This chapter surveys some of the literature on
  nondeterministic concurrency in pure functional languages, of which
  Haskell is an example. Pure functional languages are typically regarded
  as a good choice for reliable programs, but the support for correct
  concurrency is often no better, or worse, than more mainstream
  languages such as C or Java.}

\section{Avoiding Concurrency}
\label{sec:litrev-strategies}

As unrestricted concurrency leads to nondeterminism, there has been
much work on \textit{avoiding} it whilst still reaping the benefits of
parallel execution of code. Some of this work is in the guise of a
concurrency abstraction with guaranteed determinism, but others have
avoided concurrency entirely.

Perhaps the earliest such approach, \textit{evaluation strategies},
introduced by Trinder et al. (1993)\nocite{trinder}, makes use of the
basic primitives, \verb|par| and \verb|seq|, for controlling
evaluation order in Haskell. The semantics of \verb|seq| are to
evaluate its first argument and then to return its second, whereas
\verb|par| starts evaluating its first argument in parallel and
immediately returns its second. In both cases, evaluation is to
weak-head normal form (WHNF).

Strategies were intended to be powerful and extensible methods for
controlling how data structures are evaluated. A value of type
\verb|Strategy a| defines how to evaluate a value of type
\verb|a|.

\begin{minted}{haskell}
type Strategy a = a -> ()
\end{minted}

Strategies are composable, allowing constructs like so to be easily
defined:

\begin{minted}{haskell}
parList :: Strategy a -> Strategy [a]
parList _ [] = ()
parList strat (x:xs) = strat x `par` parList strat xs
\end{minted}

Hence, \verb|parList| takes a strategy to evaluate a value, and
produces a strategy to evaluate a list of such values in
parallel.

Strategies have evolved since their inception, with Marlow et
al. (2010)\nocite{strategies} redefining strategies to operate in
terms of an \textit{evaluation order monad}, called \verb|Eval|.

\begin{minted}{haskell}
type Strategy a = a -> Eval a
runEval :: Eval a -> a
\end{minted}

The monadic bind of \verb|Eval| is defined to be strict, giving a
flexible notation for expressing evaluation order without the need to
pepper code with additional applications of \verb|rseq| (the
\verb|Eval| analogue of \verb|seq|).

The existence of strategies is made possible by features of Haskell
not commonly found in other languages: without laziness, the
abstraction would have to be built around evaluating
\textit{functions} rather then \textit{data}; and without purity,
parallel side-effectful evaluation can alter the result of a
program. Despite more powerful tools being available, strategies are
still used today, provided by the
\textit{parallel}\footnote{\url{https://hackage.haskell.org/package/parallel}}
library, suggesting that the conflation of parallel evaluation and
concurrent execution found in many languages is a hindrance imposed by
the semantics of the language.

\section{Deterministic Parallel Concurrency}
\label{sec:litrev-det}

Strategies do not provide anything looking like typical concurrency,
however. They are only used to express evaluation order, and have no
explicit notion of threads. The \verb|Par| monad, by Marlow et
al. (2011)\nocite{parmonad}, is used for expressing \textit{dataflow
  parallelism}.

The \verb|Par| monad provides threads and mutable
variables. Determinism is enforced by having these mutable variables
only allow a single write, and reads block until a value has been
written. This prevents race conditions and, as no locks are provided,
deadlock is impossible.

Parallelism is implemented by running one worker thread on each
physical processor or core available. These worker threads are unable
to move between processors, and so can execute truly in parallel. This
does not mean that the \textit{entire} \verb|Par| computation will run
in parallel (if there are more \verb|Par| threads created than there
are processors, for example) but as much will execute in parallel as
possible.

Single-write shared variables are rather limiting, however, and
Lindsey et al (2014)\nocite{lvish}, noted that all that is required
for determinism is that the value a read returns must be
constant. This led to their \textit{LVish}
library\footnote{\url{https://hackage.haskell.org/package/lvish}}
which uses lattice-based shared variables. A read corresponds to
glimpsing some part of the shared mutable structure. Writes cannot
destructively update a shared structure.

LVish allows structures such as a shared mutable map. Writes may
insert values, and reads return look up a value by key (blocking until
such a key has been inserted). But one may not read the size, or the
last item added.

\begin{minted}{haskell}
helloWorld :: String
helloWorld = runPar $ do
  map <- newEmptyMap

  fork $ insert 0 "hello" map
  fork $ insert 1 "world" map

  a <- getKey 0 map
  b <- getKey 1 map
  return $ a ++ " " ++ b
\end{minted}

A rather different approach to that of \verb|Par| and LVish is taken
by Leijen et al. (2011)\nocite{revisions} in their \textit{concurrent
  revisions} library. This is strictly speaking not an example of
deterministic parallelism as it allows arbitrary I/O to occur in a
concurrent environment, but it does deterministically deal with shared
state.

Here, shared variables have a copy-on-write semantics. This makes the
\verb|fork| of concurrent revisions much more like the \verb|fork| of
the C standard library, than the \verb|fork| of the \verb|Par|
monad. When a thread is \textit{joined}, it terminates and any
modification made to shared state is integrated with the shared state
of the joining thread, by applying functions supplied when the state
was created. For example,

\begin{minted}{haskell}
counter :: Int
counter = revisioned $ do
  c <- vcreateM merge 0

  x <- fork $ do
    vmodify c (+1)
    y <- fork (vmodify c (+3))
    vmodify c (+2)
    return y

  vmodify c (+4)
  y <- join x
  join y
  vread c

  where
  merge main joinee original = main + joinee - original
\end{minted}

The final result here is 10, the sum of all the modification
operations. This is because the merge behaviour for \verb|c| is to add
any difference to its own value. Any merge behaviour at all is
possible, however as merge functions may not perform any I/O, and
threads cannot communicate other than joining each other (and a thread
cannot be joined multiple times), the whole process is deterministic.

The strength of concurrent revisions is its downside. Shared state is
deterministically updated \textit{when threads are joined}. This is
the only communication that is possible, unless other I/O is involved.

These approaches to deterministic parallelism aim to approximate a
typical concurrency abstraction. Concurrency is a powerful and useful
software structuring technique, and mere parallel evaluation isn't
enough. Sadly, any such approximations must remain approximations, as
unbridled concurrency \textit{is} nondeterministic.

\section{(Nondeterministic) Concurrency}
\label{sec:litrev-conc}

\textit{Concurrent Haskell}, by Peyton Jones at
al. (1996)\nocite{concurrent}, embellishes Haskell with a means to
start new threads, and for threads to communicate. The aim of the work
is to do with structuring programs, not with performance,

\begin{quote}
  This paper is not at all about concurrency as a means of increasing
  performance by exploiting multiprocessors. Our approach to that goal
  uses \textit{implicit}, semantically transparent, parallelism; but
  that is another story. Rather, this paper concerns the use of
  \textit{explicit}, semantically visible, concurrent I/O-performing
  processes. Our goal is to extend Haskell's usefulness into a new
  class of applications.\cite{concurrent}
\end{quote}

A new thread is started with the \verb|forkIO| function, which takes
an I/O action and begins executing it concurrently with the forking
action. Execution may not be parallel. In fact the original
implementation of Concurrent Haskell performed all execution in a
single operating system thread, scheduling internally.

The second new concept was the \verb|MVar|. An \verb|MVar| is a
mutable variable, which can either be \textit{full} or
\textit{empty}. Attempting to write to a full \verb|MVar| blocks until
it is empty, and attempting to read from an empty \verb|MVar| blocks
until it is full (and then empties it). More recent versions of the
standard library have introduced functionality to attempt
(non-blockingly) to read from or write to an \verb|MVar|, and to read
without emptying.

Despite Haskell being a pure functional language, the introduction of
multiple distinct threads of control immediately introduces the need
for inter-thread synchronisation whilst evaluating
expressions\cite{concurrent}! This is because of laziness: two threads
may try to evaluate the same thunk at the ``same time'', and
evaluating a thunk mutates the heap. So, if one thread starts to
evaluate a thunk which is already being evaluated by another, the
former needs to be paused until the latter finishes.

Threading and \verb|MVar|s are sufficient for nondeterminism, as
\verb|MVar|s can be written to multiple times and can give rise to
deadlock. Nondeterministic I/O greatly damages the ability to test
code, in Haskell as in any other language, however here it cannot be
avoided. The order of interleaving of threads is not specified as a
part of the language. It is an implementation detail, and out of reach
of the programmer.

\section{Software Transactional Memory}
\label{sec:litrev-stm}

Concurrent algorithms based on locks and mutual exclusion do not
compose. Two individually correct fragments of code may become
incorrect when composed. Consider a hash table with thread-safe insert
and remove operations. Suppose we want to move an item from one
structure to another, but without the intervening state of it not
being present in either visible. This cannot be done without
additional functionality to lock the structures preventing access by
another thread! Locking breaks abstraction, and also opens the door to
potential deadlock.

\begin{minted}{haskell}
insert :: k -> v -> Hashtable k v -> IO ()
find   :: k -> HashTable k v -> IO (Maybe v)
remove :: k -> HashTable k v -> IO ()

swap :: k -> HashTable k v -> HashTable k v -> IO ()
swap k a b = do
  val <- find k a
  remove k a
  maybe (return ()) $ flip (insert k) b
\end{minted}
%$

Transactional memory, well known in databases, was originally proposed
by Herlihey and Moss (1993)\nocite{hardwaretm} as an architectural
technique supporting lock-free data structures. A \textit{transaction}
consists of a sequence of \textit{tentative} modifications to global
state, which can be applied atomically, or aborted. Transactions are
therefore linearisable: they appear to take effect in a sequential
ordering as if one thread were orchestrating the process.

Despite the age of software transactional memory (STM), implementing
it for programming languages has proved challenging. Herlihey et al
(2003)\nocite{dstm} provided the first implementation of STM where all
of the transactions do not have to be known statically, making it
suitable for dynamically-sized data structures.

A thorn in the side of many STM implementations is the question of
safety. An STM transaction must be able to be aborted or executed
multiple times with no observable side-effects until it is committed,
but most programming languages cannot guarantee this statically. The
Haskell STM implementation, by Harris et al (2006)\nocite{haskellstm},
achieves this by defining an STM monad; the type system prevents IO
actions from being performed in the same transaction as STM actions.

To return to the hash table example, this is what it would look like
with STM in Haskell:

\begin{minted}{haskell}
insert :: k -> v -> Hashtable k v -> STM ()
find   :: k -> HashTable k v -> STM (Maybe v)
remove :: k -> HashTable k v -> STM ()

swap :: k -> HashTable k v -> HashTable k v -> STM ()
swap k a b = do
  val <- find k a
  remove k a
  maybe (return ()) $ flip (insert k) b
\end{minted}
%$

The downside of STM is that, whilst it gains safety and
linearisability, it does so at the cost of a lot of
bookkeeping. Before a transaction is committed, every variable it
reads must be checked, and the computation possibly repeated. This
means that large transactions accessing many variables can be
inefficient, but very small transactions suffer from the same
state-inconsistency problems that locks aim to solve!

\section{Systematic Concurrency Testing}
\label{sec:litrev-sct}

Systematic concurrency testing grew from efforts in
verification\cite{pbound}, where traditional model-checking techniques
are unsuitable for concurrent programs. Older techniques like
execution depth bounding are less suitable for concurrent programs, as
the depth is less predictable.

Musuvathi and Qadeer (2007) were the first to propose a heuristic
specifically for \textit{concurrent} programs: \textit{context-switch
  bounding}. Furthermore, they identified that merely bounding
\textit{pre-emptive} context switches is very powerful, as any state
(in a terminating program) can be driven to termination (or deadlock)
without incurring any pre-emptions, allowing the model checker to
explore interesting behaviours with only low bounds. Furthermore, for
a fixed number of pre-emptions, the number of possible schedules (in a
terminating program) is polynomial in the number of threads and the
execution length of the program.

The work on iterative pre-emption bounding was first implemented for
run-time testing purposes in the \textsc{Chess}\cite{heisenbugs} tool,
also by Musuvathi et al (2008).

Qadeer et al (2011) furthered this work with \textit{delay bounding},
where the number of deviations from an otherwise deterministic
scheduler (such as round-robin in order of thread creation) is
bounded. This has the advantage that the number of schedules grows
much more slowly than with pre-emption bounding. For example, there is
only one schedule with zero delays, but potentially many with zero
pre-emptions, allowing for more rapid testing.

One concern with SCT is that as concurrent programs grow, the number
of possible interleavings of operations grows much faster, resulting
in an explosion of possible schedules. There is evidence that many
concurrency bugs can be found with small test cases with few threads
and context switches\cite{pbound, dbound, empirical}, providing hope
that unit-test-like test cases can be produced and known-good
components composed efficiently. Furthermore, there has been work on
integrating results from dynamic partial-order reduction (DPOR), a
technique for determining at runtime whether a new schedule is worth
exploring, with schedule bounding\cite{bpor}.

Another concern is that test cases suitable for SCT must be
deterministic when the schedule is fixed, which often isn't easy to
achieve if external processes or network communication is
involved. Producing small suitable tests may be difficult.

\section{SCT in Functional Languages}
\label{sec:litrev-sctfunc}

Unsurprisingly, much of the work regarding concurrency testing in a
functional setting has happened in the context of Erlang. Erlang does
not have shared-memory concurrency (beyond an atomic key/value store
provided by the virtual machine) and so all communication is in the
form of message-passing. Claessen et al (2009)\nocite{pulse} developed
PULSE, a user-level scheduler for Erlang, and an automatic
instrumentation tool to convert existing programs to use PULSE. Random
scheduling was then implemented using QuickCheck, a tool for
randomised testing.

Random scheduling proved to be effective in the industrial case study
used in the paper, however it still leaves much to be desired. PULSE
was augmented with \textit{procrastination} by Arts et al (2011). The
procrastination technique looks for two possibly racey actions in a
schedule, for example two threads sending a message to a third, and
swapping the order of the actions. This proved to be even more
effective than random scheduling, and also led to easier production of
minimal failing examples.

The only Haskell application of SCT I am aware of is a blog
post\cite{typeclass} showing how some of the concurrency primitives
can be abstracted using a typeclass, and then a testing implementation
use QuickCheck in order to try to find bugs. However Stolz and Huch
(2005)\nocite{rvhaskell} do show an implementation of runtime
verification of temporal logic specifications of Haskell
programs. This, combined with an SCT scheduling technique, could
possibly be used to produce very expressive tests.

\section{Verified Concurrency}
\label{sec:litref-verify}

\begin{quote}
  Program testing can be used to show the presence of bugs, but never
  to show their absence!

  \attrib{Edsger Dijkstra (EWD249)\nocite{ewd249}}
\end{quote}

Concurrency testing can only tell us that a bug has not been found in
any of the schedules considered, but the stronger guarantee of no bug
in \textit{any possible} schedule would be much more preferable.

\textsc{Zing}, by Andrews et al (2004)\nocite{zing} is a tool and
language for verifying object-oriented concurrent programs, supporting
dynamic creation of both objects and threads for shared-memory and
message passing concurrency models. A focus of \textsc{Zing} was to
make it feasible to translate source code from real programming
languages into the \textsc{Zing} language automatically, allowing
confidence that models accurately reflect the original program.

A more comprehensive approach is taken in VCC, by Cohen et al
(2009)\nocite{vcc}, which is a verifier for complete C programs,
supporting concurrency. However, verification is not automatic, but
driven by annotations such as pre- and post-conditions and invariants
attached to program points.

Neither \textsc{Zing} nor VCC support higher-order functions, a
cornerstone of functional programming languages. D'Osualdo et al
(2013)\nocite{erlang} discuss the automated verification of
Erlang-style concurrency, by defining a core subset of Erlang with a
formal semantics. This subset includes functional constructs like
higher-order functions and recursive let bindings. Due to being based
on Erlang, it only has message passing rather than shared memory. A
tool called Soter is produced, which is capable of taking normal
Erlang programs, converting to the core language, producing a model,
and feeding into a backend verifier. Despite this impressive feat,
Soter cannot handle cases such as programs whose correctness depends
on the order of message delivery.

\section{Summary}
\label{sec:litrev-summary}

Since Trinder's strategies there have been a number of more
sophisticated models for deterministic parallel programming in the
pure functional setting. Despite this, potentially nondeterministic
concurrency is still in use. The properties of Haskell make it an
ideal setting for software transactional memory facilitating
composable atomic actions, making it easier to avoid certain classes
of bug, but this does not entirely solve the problem.

Systematic concurrency testing, and concurrency verification, are
gaining traction in the imperative and object-oriented worlds, but
application to functional languages has largely been restricted to
Erlang so far. This is unsurprising, as Erlang's major focus is
reliable concurrent programming, but given the preference of Haskell
programmers for correct software, it is perhaps odd that there has
been so little work in this setting.

Whilst some small initial efforts have been made towards
\textit{testing} in concurrent Haskell, it remains to be seen if
automated \textit{verification} of a pure concurrent functional
language is feasible, and what challenges would arise in this
application area.

One potential challenge is ``higher-order'' concurrency, where MVars
can be passed between threads through other MVars. In other settings,
one could imagine forbidding this from the language subset amenable to
auomated reasoning, as a needless complication. In a pure language,
such constructs are unavoidable in the implementation of higher-level
structures, and so much be reasoned about explicitly.
