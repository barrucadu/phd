We set out to make it easier for programmers to write correct
concurrent programs, but have we achieved that?  In this chapter we
review our contributions and draw some overall conclusions.  Recall
our contributions from \Cref{sec:intro-contributions}:

\begin{itemize}
\item A library for effectively testing Concurrent Haskell programs,
  in \Cref{chp:dejafu}.
\item An operational semantics for Concurrent Haskell, in
  \Cref{chp:dejafu}.
\item A new scheduling algorithm for randomised testing to allow
  testing programs where complete testing does not scale, in
  \Cref{chp:algorithms}.
\item A tool for discovering properties of Haskell functions operating
  on shared mutable state in the presence of concurrent interference,
  in \Cref{chp:coco}.
\end{itemize}

\paragraph{Systematic concurrency testing with rich semantics}
\Cref{chp:dejafu} introduced \dejafu{}.  This is a Haskell tool for
testing Haskell programs, but the underlying techniques are not
Haskell specific.  Haskell has an unusually rich concurrency
abstraction, whereas SCT techniques are typically described in the
literature for simple concurrency abstractions.  Even real programming
languages tend to have simple concurrency abstractions.
Maple\cite{yu2012} is able to test arbitrary pthread programs by
considering just 19 primitive actions, whereas the expression of
Haskell concurrency in \dejafu{} requires 34 just for concurrency, and
a further 9 for STM.  The number of primitive actions a concurrency
testing tool must consider is only an indirect measure of the
complexity of the concurrency model it supports, but such a large
difference is suggestive.

In Haskell, there are many different operations with partially
overlapping behaviour.  It is not clear that a typical SCT algorithm
would work effectively in this context.  Our case studies in
\Cref{sec:dejafu-casestudies} provide a convincing demonstration that
SCT can be applied to languages with rich concurrency abstractions.

\paragraph{Effective bug finding with randomised scheduling}
\Cref{chp:algorithms} introduced the swarm scheduling algorithm.  Our
benchmark results in \Cref{sec:algorithms-bench} show that it performs
as well as the PCT algorithm\cite{burckhardt2010} in terms of
bug-finding ability.  Crucially, PCT requires the user to supply
parameters derived from the program under test, whereas swarm
scheduling does not.  The freedom from any such requirement makes
swarm scheduling simpler to implement and use than PCT, yet it still
finds bugs just as effectively.

\paragraph{Discovering properties of concurrent programs}
\Cref{chp:coco} introduced CoCo.  By synthesising program terms and
performing property-based testing, CoCo can give the programmer new
insights into their code.  Like \dejafu{}, this is a Haskell tool, but
the techniques are not Haskell specific.  The underlying idea is that
we can compare sets of program behaviours to make meaningful claims
about the relation between the components which make up those
programs.  Our case studies in \Cref{sec:coco-cases} show the sorts of
properties we can discover but, as we see in
\Cref{sec:coco-conclusions}, CoCo has scaling difficulties.

The CoCo approach applies not just to concurrent programs, but to
nondeterministic programs in general.  If we have an efficient, but
nondeterministic, algorithm for a problem, we may wish to be able to
use it in place of a slow, but deterministic, algorithm.  The
deterministic algorithm is a refinement of the nondeterministic
algorithm, which may introduce additional behaviours.

\paragraph{Drawbacks of refactoring}
A weakness of our Haskell work is the \verb|MonadConc| typeclass.
Requiring programmers to modify their code, even in a straightforward
way, is a barrier to entry that many will not wish to overcome.
Furthermore, when typeclass-polymorphic code is compiled, the
definitions of typeclass member functions cannot be inlined, as they
are not known\cite{peytonjones2002}.  The recent Backpack
work\cite{yang2017} offers an alternative here, lessening the code
modification problem and solving the optimisation problem.

\paragraph{The inevitable exponentials}
Concurrent programs are nondeterministic, and this is where the
difficulty of writing correct concurrent programs comes from.
Fundamentally, testing a concurrent program requires executing it
multiple times with different schedules.  This multiplicity adds
significant overhead compared to sequential tests, where a single
execution suffices.  Even worse, a concurrent program with $n$ threads
which each execute for at most $k$ steps can have as many as
$\frac{(nk)!}{(k!)^{n}}$ executions\cite{musuvathi2007}!

\dejafu{} implements schedule
bounding\cite{emmi2011,musuvathi2008,musuvathi2007} and partial-order
reduction\cite{flanagan2005,godefroid1996} to improve the average
case, but the worst case remains a possibility.  Empirical studies
show that small test cases with just two threads and two pre-emptive
context switches suffice for finding many real-world concurrency
bugs\cite{thomson2014}.  There is a \emph{small-scope hypothesis}
here: most concurrency bugs do not only arise in complicated test
cases; rather, we just need a handful of actions to happen in the
wrong order.  This is the intuition behind PCT\cite{burckhardt2010}.
So there is a terrible asymptotic worst case, but in practice test
cases are often small.  When test cases \emph{are} too large for
systematic testing to effectively explore the state-space, then we can
use a random approach, as we did in \Cref{chp:algorithms}.

\paragraph{The difficulty of interpreting success}
It can be difficult to look at the result of a successful concurrency
test and know what it is telling us.  We saw this with CoCo in
\Cref{sec:coco-example}, where the programmer may need to run the tool
with a variety of interference functions to see the full picture.
Properties found with one sort of concurrent interference may not
generalise to cases with different interference.  Similarly,
successful concurrency tests in \dejafu{} may not generalise to cases
where the concurrent environment is different.  This difficulty is
related to the problem of judging the quality of a test suite, which
we will discuss in \Cref{chp:future_work}.

\paragraph{The difficulty of interpreting failure}
It can be difficult to look at a failing concurrency test and diagnose
the problem.  We saw this with \dejafu{} in
\Cref{sec:dejafu-casestudies-par}, where the resulting execution
traces were large and difficult to follow.  Traces are a low-level
construct: they may become invalid when library dependencies change,
even if the key scheduling decisions remain the same.  Which
information is truly important?  It is not obvious.

\paragraph{Overall conclusions}
Concurrency errors, sometimes called ``Heisenbugs'' due to their
unpredictable behaviour, can be among the most difficult to
debug\cite{musuvathi2008osdi}.  The ideas behind concurrency testing
have been around for some time now\cite{godefroid1996}, and yet
concurrency testing tools are not widely used.  By contributing new
tools and illustrating what they can do we hope to help address this
problem.
