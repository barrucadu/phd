\begin{bquote}{E. W. Dijkstra, 1972\nocite{ewd340}}
  Program testing can be a very effective way to show the presence of
  bugs, but is hopelessly inadequate for showing their absence.
\end{bquote}

\blindtext

\section{Evaluation}
\label{sec:conclusions-evaluation}

Programmers often interpret a test passing as evidence for the absence
of bugs.  Testing concurrent programs is more difficult than testing
sequential programs, for three main reasons: there is no clear quality
metric for concurrency tests; there is an exponential blow-up in
resource requirements for larger and more interesting test cases; and
correctly using a concurrency testing tool requires more insight than
a sequential testing tool.  At worst, this additional difficulty could
lead programmers to a false sense of confidence in the correctness of
their concurrent programs, by unwittingly writing poor tests.

\paragraph{Quality of a concurrency test set}
Being able to deterministically test concurrent programs is a
necessary step towards finding concurrency bugs more easily than
verification and with less disruption to users than waiting for bug
reports.  However, any testing regimen is only as good as the tests
that get written.  How do we come to believe that a testsuite is
strong evidence for the correctness of some code?

For sequential programs, we can use the traditional metric of code
coverage.  The continuous measure of coverage is difficult to
interpret, but the binary measure is not: code which is not covered at
all usually has more bugs than code which is covered by even
low-quality tests\cite{ahmed2016}.

For concurrent programs, what metric do we use?  If it is some notion
of coverage, what even is the space being covered?  There are a few
candidates:

\begin{itemize}
\item Schedule-sensitive branches are often unintentional and
  erroneous points of synchronisation between concurrent
  threads\cite{huang2015ssb}.  A good concurrency testsuite should try
  all cases in a schedule-sensitive branch.

\item Shared state which is not guarded by appropriate synchronisation
  can lead to invalid or corrupt data.  If we have functions which
  operate on some mutable state of the same type, then a good
  concurrency testsuite will check what happens when those states are
  shared and the functions executed concurrently.  If the state is a
  product type, a good concurrency testsuite will also check what
  happens when the states are only partially shared.
\end{itemize}

However, correct usage of a concurrent API often relies on assumptions
stated in documentation, cautioning users where they must incorporate
additional synchronisation.  An automated metric cannot read
documentation, and so may highlight forbidden interactions as needing
tests.

Finding an easily understood metric, like code coverage, which is both
automatic and useful in revealing legal but untested interactions, is
an open question.

\paragraph{The inevitable exponentials}
Concurrent programs are nondeterministic, and this is where the
difficulty of writing correct concurrent programs comes from.
Fundamentally, testing a concurrent program requires executing it
multiple times with different schedules.  This adds significant
overhead compared to sequential tests, where a single execution
suffices.  Even worse, the number of executions required in the worst
case is exponential in a number of different
factors\cite{musuvathi2007}.

\dejafu{} implements both schedule
bounding\cite{emmi2011,musuvathi2008,musuvathi2007} and partial-order
reduction\cite{flanagan2005,godefroid1996} to improve the average
case, but the worst case remains a possibility.  Empirical studies
show that small test cases with two threads and two pre-emptive
context switches suffice for finding many real-world concurrency
bugs\cite{thomson2014}.  So in theory there is a terrible asymptotic
worst case, but in practice test cases are often small enough for this
not to be a significant concern.

\paragraph{The interpretation game}
It can be difficult to look at a passing concurrency test and know
what it is telling us.  We see this with CoCo, where the programmer
may need to run the tool with a variety of different interference
functions to see the full picture.  Properties found with one sort of
concurrent interference may not generalise to cases with different
interference.  Similarly, passing concurrency tests in \dejafu{} may
not generalise to cases where the concurrent environment is different.
This is a difficulty which would be solved by being able to measure
the quality of a concurrency test set: a measure would highlight
weaknesses in passing tests, and help avoid overly generous
interpretations.

It can be difficult to look at a failing concurrency test and diagnose
the problem.  We see this with \dejafu{}, where the resulting
execution traces can expose many low-level details of the
implementation of library functions.  Which information is truly
important is not obvious.  Traces may become invalid when library
dependencies are upgraded, even if the important scheduling points
remain the same.  The \textsc{Concurrit}\cite{elmas2013} tool offers
an alternative approach here, where buggy schedules are represented by
a small collection of constraints capturing the key scheduling
decisions.  Although intended for reproducing bugs, \textsc{Concurrit}
constraints could be a much simpler way to present faulty traces.

\section{Contributions}
\label{sec:conclusions-contributions}

\paragraph{Systematic concurrency testing with rich semantics}
\dejafu{} and CoCo are Haskell tools for testing Haskell programs, but
the techniques they use are not limited to Haskell.  Systematic
concurrency testing techniques are typically described in the
literature for small languages with simple concurrency abstractions.
Even real programming languages tend to have simple concurrency
abstractions.  For example, Maple\cite{yu2012} is able to test
arbitrary pthread programs by considering just 19 primitive actions,
whereas the expression of Haskell concurrency in \dejafu{} requires 33
(not counting STM).

Haskell has an unusually rich concurrency abstraction.  There are many
different operations with partially overlapping behaviour.  It is not
clear that a typical SCT algorithm would work effectively in this
context.  \dejafu{} provides a convincing demonstration that SCT can
be applied to languages with a rich concurrency abstraction.  This is
an important stepping-stone to implementing SCT tools for other
similarly rich languages.
