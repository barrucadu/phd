\chapstart Testing concurrent programs cannot be done with conventional
techniques.  The nondeterminism of scheduling means that a test may produce
different results in different executions.  In this chapter we give an
introduction to \emph{systematic} concurrency testing, which addresses this
problem.  We first give a high-level overview~\sref{sct-fundamentals}, then have
a more detailed discussion of specific implementation
approaches~(\S\ref{sec:sct-complete}~and~\S\ref{sec:sct-incomplete}).

\section{Controlling Program Execution}
\label{sec:sct-fundamentals}

Systematic concurrency testing
(SCT)\cite{coons2013,musuvathi2008,musuvathi2007,flanagan2005} is a family of
techniques for testing concurrent programs.  SCT works by repeatedly executing
the program with different schedules to try to find bugs.  It is
\emph{systematic} because this exploration is generally not random, but follows
some deterministic search pattern.

SCT can be implemented by overriding the concurrency primitives of the
programming language, forcing all threads to run on one controlling thread, and
making scheduling decisions between atomic blocks.  An atomic block consists of
a number of thread-local operations followed by a single access to shared state.
This is preferable to exploring all possible points for making scheduling
decisions, as the order of interleaving of thread-local operations cannot affect
the result.

Typically the assumption is made that all executions are \emph{terminating}: all
possible sequences of scheduling decisions will lead to a termination by
deadlock or otherwise.  Another common assumption is that there is a
\emph{finite} number of possible schedules: this forbids finite but arbitrarily
long executions, as can be created with constructs such as spinlocks.

Systematic testing terminates when there are no further distinct schedules.

\section{Sacrificing Time for Certainty}
\label{sec:sct-complete}

Dynamic partial-order reduction (DPOR)\cite{flanagan2005,godefroid1996} is a
\emph{complete} approach to SCT\@.  It is based on the insight that, when
constructing schedules, we only need to consider different orderings of a pair
of actions if the order in which they are performed could affect the result of
the program:

\begin{displayquote}[Flanagan and Godefroid, 2005]
  Let $\mathcal T$ be the set of transitions in a concurrent system.  A binary,
  reflexive, and symmetric relation
  $\mathcal D \subseteq \mathcal T \times \mathcal T$ is a valid
  \emph{dependency relation} iff, for all $t_{1}, t_{2} \in \mathcal T$,
  $(t_{1}, t_{2}) \notin \mathcal D$ ($t_{1}$ and $t_{2}$ are
  \emph{independent}) the following properties hold for all program states $s$:

  \begin{enumerate}
  \item if $t_{1}$ is enabled in $s$ and $s \xrightarrow{t_{1}} s'$, then
    $t_{2}$ is enabled in $s$ iff $t_{2}$ is enabled in $s'$; and

  \item if $t_{1}$ and $t_{2}$ are enabled in $s$, then there is a unique state
    $s'$ such that $s \xrightarrow{t_{1}t_{2}} s'$ and
    $s \xrightarrow{t_{2}t_{1}} s'$.
  \end{enumerate}
\end{displayquote}

In other words, independent transitions cannot enable or disable each other, and
enabled independent transitions commute.  Rather than use this relational
definition directly, typically some sufficient conditions for dependency are
identified.  These conditions are determined by what sorts of things the
concurrent system under test can express.

Typically, the presentation of algorithms assumes a simple core concurrent
language of just reads and writes.  This gives rise to the following dependency
condition:

\begin{align*}
  x \dependent y \iff& \mathrm{thread\_id}(x) = \mathrm{thread\_id}(y) \lor\\
    &\left(\mathrm{variable}(x) = \mathrm{variable}(y)
     \land \left(\mathrm{is\_write}(x) \lor \mathrm{is\_write}(y)\right)\right)
\end{align*}

Where $x \dependent y$ is read as ``$x$ and $y$ are dependent''.  This choice of
notation would suggest a symbol $\leftrightarrow$ meaning independence, but that
doesn't seem to be used in the literature.

The dependency relation for Haskell is rather more complex than this, as there
are more actions than just reads and writes.  However it can be simplified to a
few general conditions over different sorts of reads and writes, with some
remaining special cases for software transactional memory and exceptions.
Furthermore, as a Haskell program terminates when the main thread terminates,
there is a dependency between the last action in a trace (whatever it may be)
and \emph{everything} else.  The dependency relation used in \dejafu{} is given
in~\sref{dejafu-testing}.

Characterising the execution of a concurrent program by the ordering of its
dependent actions gives us a \emph{partial order} over the actions in the entire
program.  An execution trace may be just one possible \emph{total} order
corresponding to the same partial order.  The goal of partial-order reduction,
then, is to eliminate these redundant total orders by intelligently making
scheduling decisions to permute the order of dependent actions.

This can be done by executing a program with a deterministic scheduler, and then
examining the trace for \emph{backtracking points}.  A backtracking point is a
place in the execution where multiple dependent choices were available, and only
one was tried.  The exploration of the state space continues by making the same
scheduling decisions up to that point, and then making a different decision.
This process of doing partial-order reduction based on information gathered at
run-time, rather than static analysis, is called \emph{dynamic partial-order
  reduction} (DPOR).

\subsection{Integration with Relaxed Memory}

A simple buffering technique is used to model the nondeterminism of
unsynchronised writes under total and partial store order\cite{zhang2015}:

\begin{itemize}
\item Under total store order, each thread has a queue of buffered writes.
\item Under partial store order, each thread has one queue of buffered writes
  per shared variable.
\end{itemize}

A buffered write is only visible to the thread which made it.  When some
synchronising operation is performed, such as \verb|readMVar| in Haskell, all
buffers are flushed.

Individual buffered writes can be committed nondeterministically.  To model
this, we introduce one additional \emph{phantom thread} for each nonempty
buffer.  When scheduled, a phantom thread commits the top write from its buffer.

SCT techniques assume there is only one source of nondeterminism: the scheduler.
If a second source is added, such as when writes are committed, it is difficult
to integrate this with existing algorithms.  By using phantom threads, the two
sources of nondeterminism are unified, and existing algorithms just work.

\section{Sacrificing Certainty for Time}
\label{sec:sct-incomplete}

Schedule bounding\cite{emmi2011,musuvathi2008,musuvathi2007} is an
\emph{incomplete} approach to SCT\@.  A \emph{bound function} is defined which
associates a schedule with some integer value.  This function is monotonically
increasing over schedule prefixes: if a schedule has an associated value of $n$,
all its prefixes will have an associated value $\leq n$.  This value $n$ is
limited by some pre-determined bound.

A common schedule bounding approach is \emph{pre-emption
  bounding}\cite{musuvathi2007}, which limits the number of pre-emptive context
switches.  Empirical evidence\cite{thomson2014} shows that small bounds, and
small numbers of threads, are effective for finding bugs in many real-world
programs.

Another common approach is \emph{fair bounding}\cite{musuvathi2008}, which
prevents infinitely long executions when constructs such as spinlocks are used.
It bounds the difference between how many times any two threads may explicitly
yield.

Strictly speaking, schedule bounding refers to trying only those schedules with
a bound value equal to some fixed parameter.  A variant of this is
\emph{iterative} bounding, where this parameter is increased from zero up to
some limit.  Another variant is where an inequality, rather than an equality, is
used.  This explores the same schedules as iterative bounding, but doesn't
impose the same ordering properties over schedules tried.  In practice,
``schedule bounding'' typically refers to this third type, unless specified
otherwise.

\subsection{Integration with Dynamic Partial-order Reduction}

The na\"{\i}ve way to integrate DPOR with schedule bounding would be
to first use partial-order techniques to prune the search space, and
then to additionally filter things out with schedule bounding.

However, this is unsound.  This approach misses parts of the search space
reachable within the bound.  This is because the introduction of the bound
introduces new dependencies between actions, which cannot be determined \emph{a
  priori}.  The solution is to add \emph{conservative} backtracking points to
account for the bound in addition to any normal backtracking points that are
identified.  Where to insert these depends on the bound function.

In the case of pre-emption bounding, it suffices to try all possibilities at the
last context switch before a normal backtracking point.  This is because context
switches influence the number of pre-emptions needed to reach a given program
state, depending on which thread gets scheduled.  In practice this tends not to
greatly increase the search space.
