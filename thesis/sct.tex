Testing concurrent programs cannot be done with conventional techniques.  The
nondeterminism of scheduling means that a test may produce different results in
different executions.  In this chapter we give an introduction to concurrency
testing through \emph{controlled scheduling}, which addresses this problem.  We
first give a high-level overview~\sref{sct-fundamentals}, then have a more
detailed discussion of specific implementation approaches, both
complete~\sref{sct-dpor} and incomplete~\sref{sct-bounding}.

\section{Controlling Program Execution}
\label{sec:sct-fundamentals}

With a controlled scheduling technique, execution of a program is serialised and
the controlling scheduler drives the program.  Program schedules are either
explored \emph{systematically}\cite{coons2013,musuvathi2008,musuvathi2007,flanagan2005}
(often called ``systematic concurrency testing'' or ``SCT'') or
randomly\cite{thomson2016,burckhardt2010}.  Non-controlled methods do not
provide their own scheduler, and instead use delays to affect
execution\cite{yu2012}.  Controlled scheduling techniques are attractive because
of their ability to record and replay program executions.

Systematic techniques may be complete, whereas randomised techniques are
typically not.  This means that random techniques cannot ``complete'', even for
small programs, and are usually run for some predetermined number of program
executions.

When exploring schedules, we only care about the relative ordering of visible
actions, those actions of a thread which may affect another.  Controlled
scheduling techniques can be implemented by overriding the concurrency
primitives of the programming language\cite{walker2015}; by instrumenting the
source program\cite{claessen2009}; or by instrumenting the compiled
program\cite{yu2012,musuvathi2006}.

Typically we assume that all executions are \emph{terminating}: all possible
sequences of scheduling decisions will lead to a termination by deadlock or
otherwise.  Another common assumption is that the number of possible schedules
is \emph{finite}: forbidding finite but arbitrarily long executions, as can be
created with constructs such as spinlocks.  We can sacrifice completeness to do
away with these assumptions, as we shall see in \cref{sec:sct-bounding}.

\section{Dynamic Partial-order Reduction}
\label{sec:sct-dpor}

Dynamic partial-order reduction (DPOR)\cite{flanagan2005,godefroid1996} is a
\emph{complete} approach to SCT\@.  It is based on the insight that, when
constructing schedules, we only need to consider different orderings of a pair
of visible actions if the order in which they are performed could affect the
result of the program.  We call this relation between actions
the \emph{dependency relation}:

\begin{bquote}{Flanagan and Godefroid, 2005\nocite{flanagan2005}}
  Let $\mathcal T$ be the set of transitions in a concurrent system.  A binary,
  reflexive, and symmetric relation $\mathcal D \subseteq \mathcal
  T \times \mathcal T$ is a valid dependency relation iff, for all $t_{1},
  t_{2} \in \mathcal T$, $(t_{1}, t_{2}) \notin \mathcal D$ ($t_{1}$ and $t_{2}$
  are independent) the following properties hold for all program states $s$:

  \begin{enumerate}
  \item if $t_{1}$ is enabled in $s$ and $s \xrightarrow{t_{1}} s'$, then
    $t_{2}$ is enabled in $s$ iff $t_{2}$ is enabled in $s'$; and

  \item if $t_{1}$ and $t_{2}$ are enabled in $s$, then there is a unique state
    $s'$ such that $s \xrightarrow{t_{1}t_{2}} s'$ and
    $s \xrightarrow{t_{2}t_{1}} s'$.
  \end{enumerate}
\end{bquote}

In other words, independent transitions cannot enable or disable each other, and
enabled independent transitions commute.  When implementing DPOR, we typically
identify the conditions necessary for dependency, rather than work with this
relational definition directly.  These conditions are determined by what sorts
of things the concurrent language in use can express.

Typically, the presentation of algorithms assumes a simple core concurrent
language of just reads and writes.  This gives rise to the following dependency
relation:

\begin{align*}
  x \dependent y \iff& \mathrm{thread\_id}(x) = \mathrm{thread\_id}(y)~\lor\\
    &\left(\mathrm{variable}(x) = \mathrm{variable}(y)
     \land \left(\mathrm{is\_write}(x) \lor \mathrm{is\_write}(y)\right)\right)
\end{align*}

Where $x \dependent y$ is read as ``$x$ and $y$ are dependent''.  This choice of
notation would suggest a symbol $\leftrightarrow$ meaning independence, but that
doesn't seem to be used in the literature.  This dependency relation can be read
as ``$x$ and $y$ are dependent if and only if they are actions in the same
thread, or they are actions involving the same variable where at least one is a
write.''

The dependency relation for Haskell is rather more complex than this, as there
are more actions than just reads and writes.  However it can be simplified to a
few general conditions over different sorts of reads and writes, with some
remaining special cases for software transactional memory and exceptions.
Additionally, a Haskell program terminates when the main thread terminates,
which complicates matters further: a dependency relation isn't enough to express
this behaviour.  The dependency relation used in \dejafu{} is given
in~\sref{dejafu-testing}.

\subsection{Total and Partial Orders}

Characterising the execution of a concurrent program by the ordering of its
dependent actions gives us a \emph{partial order} over the actions in the entire
program.  An execution trace may be just one possible \emph{total} order
corresponding to the same partial order.  The goal of partial-order reduction,
then, is to eliminate these redundant total orders by intelligently making
scheduling decisions to permute the order of dependent actions.

\emph{Dynamic} partial-order reduction is so called because it gathers
information about the dependencies between threads at run-time, to avoid the
imprecision of static analyses\cite{flanagan2005}.  It works by executing the
program until completion, making arbitrary choices to resolve scheduler
nondeterminism, dynamically collecting information about how threads have
communicated during this specific execution.  This execution trace is then
examined to add backtracking points along the trace that identify alternative
scheduling decisions that need to be explored because they might lead to other
executions which correspond to a different partial-order.  DPOR is complete.
When it completes, all possible results of the program are guaranteed to have
been detected.  The algorithm repeats until all backtracking points have been
explored and no new ones are found, because of this it only works if all
executions are terminating and the number of distinct executions is finite.

\subsection{Integration with Relaxed Memory}

In the name of performance, modern processors often implement memory models that
are weaker than sequential consistency\cite{lamport1979} by using optimisations
such as speculative execution, buffering, and caching.  Unlike sequential
consistency, where a concurrent program behaves as a simple interleaving of
atomic thread actions, relaxed memory models can be more complex, making program
analysis and debugging difficult.  Under Total Store Order (TSO), which x86
processors use\cite{owens2009}, reads and writes in the same thread to different
memory locations may be re-ordered.  Under Partial Store Order (PSO), a
relaxation of TSO\cite{sparc}, two writes in the same thread, but to different
memory locations, may also be reordered.

A simple buffering technique can be used model the nondeterminism of
unsynchronised writes under TSO and PSO\cite{zhang2015}:

\begin{itemize}
\item Under TSO, each thread has a queue of buffered writes.
\item Under PSO, each thread has a queue of buffered writes for each shared
variable.
\end{itemize}

A buffered write is only visible to the thread which made it, if a thread has no
buffered write, it reads the most recently committed value of the variable.
Buffered writes are committed nondeterministically.  To model this, we introduce
one additional \emph{phantom thread} for each nonempty buffer.  When scheduled,
a phantom thread commits the top write from its buffer.

SCT techniques assume that there is only one source of nondeterminism: the
scheduler.  If a second source is added, such as when writes are committed, it
is difficult to integrate this with existing algorithms.  By using phantom
threads, the two sources of nondeterminism are unified, and existing algorithms
just work\cite{zhang2015}.

\subsection{Maximal Causality Reduction}

Maximal Causality Reduction (MCR)\cite{huang2017,huang2015} is an alternative to
DPOR which explores a provably minimal number of executions.  Consider these
three threads:

\begin{center}
\verb|p: write x       q: write x      r: read x|
\end{center}

All three actions are dependent, and so DPOR would explore all interleavings:
\texttt{pqr}, \texttt{prq}, \texttt{qpr}, \texttt{qrp}, \texttt{rpq}, \texttt{rqp}.
However, if we consider what value \texttt{r} reads, many of these are
equivalent.  \texttt{pqr} results in the same value being read as \texttt{qrp}.
In fact we only need to explore half of the interleavings to find all the
distinct values read.  As program execution is driven by what values different
threads read (an unread write changes nothing), ideally we would only try a
schedule if it leads to at least one distinct value being read.

The MCR algorithm is similar in outline to DPOR\@.  It performs an execution
(resolving scheduling nondeterminism arbitrarily), gathers a trace including
information about the thread communication, and then uses this trace to compute
new schedule prefixes.  The difference is that these schedule prefixes ensure
that at least one read produces a previously unseen value.  MCR uses the trace
to compute a model of program behaviour as a set of quantifier-free first-order
logical formulae.  These formulae can then be augmented with a state-change
requirement and given to an SMT solver, such as z3, to produce new schedule
prefixes.  When executed on benchmark programs, MCR outperforms DPOR by orders
of magnitude\cite{huang2017}.

MCR imposes one additional restriction however, which may render it unsuitable
for Haskell.  MCR requires a concurrency model to be \emph{locally
deterministic}\cite{huang2015}.  That is, only the previous actions of a thread
(and not actions of other threads) determine the next action of the thread,
although if that action is a read then it is allowed to get its value from the
latest write.  This is not the case for Haskell, where one thread may be killed
by another by throwing an exception to it.  However, it may be possible to encode Haskell
exceptions in an MCR-friendly way by giving each thread an ``exception
variable'', and inserting reads to this variable before every normal action, but
some difficulty remains, as even blocked threads may be interrupted by
exceptions in Haskell.

Like DPOR, MCR can be extended to support the relaxed memory models TSO and
PSO\cite{huang2016}.

\section{Schedule Bounding}
\label{sec:sct-bounding}

Schedule bounding\cite{emmi2011,musuvathi2008,musuvathi2007} is an
\emph{incomplete} approach to concurrency testing.  A \emph{bound function} is
defined which associates a sequence of scheduling decisions with some value of a
type that has a total order, such as the integers.  This function is
monotonically increasing: if some sequence has an associated value of $n$, all
its prefixes will have an associated value of at most $n$.  This value $n$ is
limited by some pre-determined bound.  Testing proceeds by executing all
schedules within the bound.

A common schedule bounding approach is pre-emption bounding\cite{musuvathi2007},
which limits the number of pre-emptive context switches.  Empirical
evidence\cite{thomson2014} shows that small bounds, and small numbers of
threads, are effective for finding bugs in many real-world programs.  Another
common approach is fair bounding\cite{musuvathi2008}, which bounds the
difference between how many times any two threads may explicitly yield.  This
prevents infinitely long executions when using constructs such as spinlocks,
which loop until some condition is true, yielding on every iteration.

Bound functions can be combined, where a sequence of scheduling decisions is
outside the combined bound if it is outside either of the constituent bounds.

Strictly speaking, schedule bounding refers to trying only those schedules with
a bound value equal to some fixed parameter.  A variant of this is
\emph{iterative} bounding, where this parameter is increased from zero up to
some limit\cite{musuvathi2007}.  Another variant is where an inequality, rather
than an equality, is used.  This explores the same schedules as iterative
bounding, but doesn't impose the same ordering properties over schedules tried.
In practice, ``schedule bounding'' typically refers to this third type, unless
specified otherwise.

\subsection{Integration with DPOR}

Schedule bounding can be combined with DPOR to produce a technique which is
complete within its bound.  The na\"{\i}ve way to integrate these techniques
would be to first use partial-order techniques to prune the search space, and
then to additionally filter things out with schedule bounding.  However, this is
unsound.  This approach misses parts of the search space reachable within the
bound.  This is because the introduction of the bound introduces new
dependencies between actions, which cannot be determined \emph{a
  priori}\cite{coons2013}.

The solution is to add \emph{conservative} backtracking points to account for
the bound in addition to any normal backtracking points that are identified.
Where to insert these depends on the bound function.

In the case of pre-emption bounding, it is sufficient to try all possibilities
at the last context switch before a normal backtracking point\cite{coons2013}.
This is because context switches influence the number of pre-emptions needed to
reach a given program state, depending on which thread gets scheduled.  In
practice this tends not to greatly increase the search space.
