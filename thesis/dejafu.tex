\begin{displayquote}
  % hack to stop the displayquote from eating the [
  {[}\dejafu{} is] A martial art in which the user's limbs move in
  time as well as space, [\ldots] It is best described as ``the
  feeling that you have been kicked in the head this way
  before.''\cite{pratchett2001}
\end{displayquote}

\noindent
Specialised tools are necessary to test concurrent programs.  In this
chapter we present and evaluate \dejafu{}, our library for testing
concurrency in Haskell.  We discuss the scope of the
tool~\sref{dejafu-scope} and present our abstraction over the GHC
Haskell concurrency functionality~\sref{dejafu-monadconc}.  We then
show an example of a small logic puzzle which we can represent as a
concurrent program~\sref{dejafu-100}.  We explain how programs using
our abstraction are executed~\sref{dejafu-execution} and
tested~\sref{dejafu-testing}, and argue the correctness of the testing
approach~\sref{dejafu-correctness}.  We present three case
studies~\sref{dejafu-casestudies}, and finally evaluate our
results~\sref{dejafu-evaluation}.

This chapter is derived from our previous work \cite{YCS-2016-503} and
\cite{walker2015}.

\section{Scope}
\label{sec:dejafu-scope}

We aim to support most of the functionality of GHCâ€™s concurrency API,
as made available through the
Control.Concurrent\cite{control_concurrent} and
Control.Exception\cite{control_exception} module hierarchies.
However, we cannot test things which unavoidably require support from
the runtime system.  In particular, we do not support:

\begin{itemize}
\item Operations to block a thread until a file descriptor becomes
  available, as this introduces an additional source of
  nondeterminism.
\item Operations to query which capability (OS thread) a Haskell
  thread is running on, as this introduces an additional source of
  nondeterminism.
\item Automatically detecting if a thread is deadlocked on an
  \verb|MVar| or \verb|TVar| and throwing an exception to it, as we
  cannot reliably detect deadlock involving only a subset of threads
  without support from the garbage collector.
\end{itemize}

We also do not yet support \emph{bound threads}: a Haskell thread
which will always run on the same, unique, OS thread.  Bound threads
are essential for using the foreign function interface (FFI) to call C
libraries which use thread-local state, to ensure the Haskell thread
always sees its state and never the state of another thread.  We have
a prototype implementation, which is planned to be included in the
next major release of
\dejafu{}\footnote{\url{https://github.com/barrucadu/dejafu/issues/126}}.

There is more to \verb|IO| than concurrency and exceptions.  \dejafu{} supports
testing computations with embedded \verb|IO| actions provided that the
programmer ensures that the \verb|IO| action is atomic; that it is deterministic
when executed with a fixed schedule; and that it does not block on the action of
another thread.  Failing to meet any of these conditions may lead to incomplete
testing.

\paragraph{Semantic departures}
In the functionality we do support, we model behaviour as close as
reasonably possible to GHC.  We make a few departures from the
traditional semantics where there is good reason to do so:

\begin{itemize}
\item The \verb|getNumCapabilities| operation allows the programmer to
  query the number of capabilities.  During testing, we return two,
  despite executing everything in the same OS thread.  This is to
  avoid special-case behaviour for one capability, which may reduce
  concurrency.
\item Runtime errors, such as pattern match failures, can be caught as
  exceptions inside \verb|IO|.  As there is no non-\verb|IO| way to do
  the same, \dejafu{} cannot catch these errors.
\item The \verb|threadDelay| operation is required to yield the
  thread, but not necessarily to delay it.  This is because it is not
  clear how to incorporate time into the testing model.
\end{itemize}

\section{Abstracting over Concurrency}
\label{sec:dejafu-monadconc}

There are three ways of implementing a concurrency testing tool:
overriding the concurrency primitives of the language; instrumenting
the source program; or instrumenting the compiled program.  We adopt
the first approach in \dejafu{}.  Haskell's typeclass machinery lets
us specify an interface for concurrency, and to provide different
concrete implementations.  There is one implementation using the
\verb|IO| type and the standard functions; there is another using our
own type, based on continuations which we can inspect.

\begin{listing}
\centering
\begin{cminted}{haskell}
class (Monad m, {- other constraints omitted -}) => MonadConc m where
  type MVar m :: * -> *
  -- other types omitted

  newEmptyMVar :: m (MVar m a)
  newEmptyMVar = newEmptyMVarN ""

  newEmptyMVarN :: String -> m (MVar m a)
  newEmptyMVarN _ = newEmptyMVar

  putMVar  :: MVar m a -> a -> m ()
  readMVar :: MVar m a -> m a
  takeMVar :: MVar m a -> m a
  -- other operations omitted
\end{cminted}
\caption{A fragment of the \texttt{MonadConc} typeclass.}\label{lst:monadconc}
\end{listing}

We call our typeclass \verb|MonadConc| for monads which do
concurrency.  \Cref{lst:monadconc} shows a fragment.  When defining an
instance of this class, the programmer must supply concrete types for
the abstract types in the interface.  They must also supply
implementations of, at least, all undefined operations.  Some
operations have default definitions: for example, there are two ways
of constructing an empty \verb|MVar|.  One way takes a name, which is
displayed in debugging information, the other does not.  Each has a
default definition in terms of the other, and so the programmer must
supply at least one.

\begin{listing}
\centering
\begin{cminted}{haskell}
instance Monad n => MonadConc (ConcT r n) where
  type MVar (ConcT r n) = MVar r
  -- other types omitted

  newEmptyMVarN n = toConc (ANewMVar n)

  putMVar  var a = toConc (\c -> APutMVar var a (c ()))
  readMVar var   = toConc (AReadMVar var)
  takeMVar var   = toConc (ATakeMVar var)
  -- other operations omitted
\end{cminted}
\caption{A fragment of the \texttt{MonadConc} testing implementation.}\label{lst:mvarops}
\end{listing}

The type for our testing implementation is called \verb|ConcT r n|, which is a
monad that has access to \emph{references} of type \verb|r| in a monad of type
\verb|n|.  \Cref{lst:mvarops} shows the instance of \verb|MonadConc| for
this type.  Each concurrency operation is of the same form: we take the
arguments and wrap them up inside a data structure whose final argument is a
continuation, which is then converted into a \verb|ConcT| value.

A concurrent computation is just a large value.  We can inspect each
`step' of the computation by looking at the data constructor used.  We
call these constructors \emph{primitive actions}.  With these actions
we express the operations in the \verb|MonadConc| class.  A full
listing of primitive actions is available in \Cref{app:primops}.

\section{The $N$ Prisoners Problem}
\label{sec:dejafu-100}

There's a popular logic puzzle which goes something like this:

\begin{displayquote}
  There are $N$ prisoners in solitary cells.  There's a central living
  room with one light bulb.  No prisoner can see the light bulb from
  their own cell.  Everyday, the warden picks a prisoner equally at
  random, and that prisoner visits the living room.  While there, the
  prisoner may toggle the bulb.  The prisoner also has the option of
  asserting that all $N$ prisoners have been to the living room.  If
  this assertion is false, all $N$ prisoners are shot.  However, if
  true, all prisoners are set free.  Thus, the assertion should only
  be made if the prisoner is 100\% certain of its validity.  The
  prisoners are allowed to get together one night in the courtyard, to
  discuss a plan.  What plan should they agree on, so that eventually,
  someone will make a correct assertion?
\end{displayquote}

We can express this puzzle as a concurrency problem: the warden is the
scheduler, each prisoner is a thread, and when the program terminates
every prisoner should have visited the living room.  So if every
thread (prisoner) is scheduled (taken to the room), then the prisoners
are successful.  \dejafu{} can give us execution traces.  So, given
some way of setting up the prison, we can ask \dejafu{} to execute it
and then examine the returned traces to discover if the prisoners are
successful.

\subsection{The Probabilistic Solution}

One school of thought says to just wait for $10 N$ days, because by
then it's unlikely that any single prisoner has never visited the
room.  \Cref{lst:100good} shows an implementation of this strategy,
and \Cref{tbl:100rand} shows how the prisoners fare over 100 random
executions.

\begin{table}
  \centering
  \begin{tabular}{l|rrrrrrrr} \toprule
    Prisoners          &   1 &   2    &   3    &   4    &   5    &   6    &   7    &   8 \\
    Successes          & 100 & 100    & 100    & 100    & 100    & 100    & 100    & 100 \\
    Failures           &   0 &   0    &   0    &   0    &   0    &   0    &   0    &   0 \\
    Avg.\@ Room Visits &   2 &  18.35 &  31.92 &  43.52 &  55.88 &  67.37 &  77.05 &  90.40 \\ \bottomrule
  \end{tabular}
  \caption{The behaviour of the probabilistic solution.}\label{tbl:100rand}
\end{table}

After waiting $10 N$ days, the chance that any one prisoner has been
consistently missed is $\left(1 - \frac{1}{n}\right)^{10n}$.  This
converges to $\frac{1}{e^{10}}$.

\subsection{The Perfect Solution}

Perhaps our prisoners are more cautious, and even a small chance of
death is too much.  They want to be certain of their success.  A slow
but simple strategy is for the prisoners to nominate a leader.  Only
the leader can declare to the warden that everyone has visited the
room.  Whenever a prisoner other than the leader visits the room, if
this is their first time in the room with the light off, they turn it
on, otherwise they do nothing.  Whenever the leader enters the room,
they turn the light off.  When the leader has turned the light off 99
times, they tell the warden that everyone has visited.
\Cref{lst:100perfect} shows an implementation of these behaviours.

\begin{listing}
\begin{sublisting}{\textwidth}
\centering
\begin{cminted}{haskell}
leader :: MonadConc m => Int -> TVar (STM m) Int -> m ()
leader numPrisoners days = atomically $ do
  numDays <- readTVar days
  when (numDays < (numPrisoners - 1) * 10) retry

notLeader :: MonadConc m => TVar (STM m) Int -> m ()
notLeader days = forever $ atomically (modifyTVar days (+1))

prison :: MonadConc m => Int -> m ()
prison numPrisoners = do
  days <- atomically (newTVar 0)
  for_ [1..numPrisoners-1] (\_ -> fork (notLeader days))
  leader numPrisoners days
\end{cminted}
\caption{The probabilistic solution: just wait a long time and gamble.}\label{lst:100good}
\end{sublisting}

% [layout hack]: no gap between the listings otherwise
\vspace{2.5em}

\begin{sublisting}{\textwidth}
\centering
\begin{cminted}{haskell}
data Light = IsOn | IsOff

leader :: MonadConc m => Int -> TVar (STM m) Light -> m ()
leader numPrisoners light = go 0 where
  go counter = do
    counter' <- atomically $ do
      state <- readTVar light
      case state of
        IsOn -> do
          writeTVar light IsOff
          pure (counter + 1)
        IsOff -> retry
    when (counter' < prisoners - 1)
      (go counter')

notLeader :: MonadConc m => TVar (STM m) Light -> m ()
notLeader light = do
  atomically $ do
    state <- readTVar light
    case state of
      IsOn  -> retry
      IsOff -> writeTVar light IsOn
  forever yield

prison :: MonadConc m => Int -> m ()
prison numPrisoners = do
  light <- atomically (newTVar IsOff)
  for_ [1..numPrisoners-1] (\_ -> fork (notLeader light))
  leader numPrisoners light
\end{cminted}
\caption{The perfect solution: nominate a leader, who waits until they are certain that everyone has been in the room.}\label{lst:100perfect}
\end{sublisting}
\caption{Two solutions for the $N$ prisoners problem.}\label{lst:100sols}
\end{listing}

We can satisfy ourselves that this solution works for all cases by
using \dejafu{}'s systematic concurrency testing functionality, which
is a combination of dynamic partial-order reduction and schedule
bounding.  \Cref{tbl:100slow} shows how the number of schedules
explored and average number of room visits grows as the number of
prisoners increases.  It does not scale well.

This algorithm is something of a worst-case for DPOR.  Every thread is
messing with the same shared state, so DPOR has to try every
interleaving.  Taking another look at our prisoners, we can see two
things which a human would use to decide whether some schedules are
redundant or not:

\begin{table}
\begin{subtable}{\textwidth}
  \centering
  \begin{tabular}{lrrr} \toprule
    Prisoners          & 1 & 2 &    3 \\
    Schedules          & 1 & 5 & 2035 \\
    Avg.\@ Room Visits & 2 & 7 &  133 \\ \bottomrule
  \end{tabular}
  \caption{Using \dejafu{}'s default schedule bounds.}\label{tbl:100slow}
\end{subtable}

% [layout hack]: no gap between the tables otherwise
\vspace{2.5em}

\begin{subtable}{\textwidth}
  \centering
  \begin{tabular}{lrrrrrr} \toprule
    Prisoners          & 1 & 2 & 3   &  4   &    5 &      6 \\
    Schedules          & 1 & 1 & 4   & 48   & 1536 & 122880 \\
    Avg.\@ Room Visits & 2 & 4 & 7.5 & 11.5 &   16 &     21 \\ \bottomrule
  \end{tabular}
  \caption{Using a custom fair bound to prevent yields.}\label{tbl:100fast}
\end{subtable}
\caption{How the number of schedules grows with increasing prisoner numbers.}\label{tbl:100slowfast}
\end{table}

\begin{enumerate}
\item If we adopt any schedule other than alternating leader /
  non-leader, threads will block without doing anything.  So we should
  alternate.
\item When a non-leader has completed their task, they will always
  yield.  So we should never schedule a prisoner who will yield.
\end{enumerate}

\dejafu{} can't make use of (1).  This optimisation could possibly be
inferred \emph{if} \dejafu{} were able to compare values inside
\verb|TVar|s.  We cannot do that without putting an \verb|Eq|
constraint on \verb|writeTVar|, however.  \dejafu{} \emph{can} make
use of (2).  \dejafu{} already bounds the maximum number of times a
thread can yield, so that it can test constructs like spinlocks.  This
is called \emph{fair bounding}.  The default bound is five, but if we
set it to zero \dejafu{} will never schedule a thread which is going
to yield.  \Cref{tbl:100fast} shows how the number of schedules
explored and average number of room visits grow with this change.

This is better, but still scales poorly.  The threads are all mutating
the same bit of shared state, which is a bad case for DPOR.  This is
probably as good as we can do without adding some extra primitives to
\dejafu{} to optimise the case where we have an \verb|Eq| instance
available, or by using an alternative systematic testing algorithm.
In \Cref{chp:algorithms} we will discuss an alternative
\emph{incomplete} approach to this, and other, concurrency problems.

It's not the end for DPOR, however.  Empirical
studies\cite{thomson2014} have found that many concurrency bugs can be
exhibited with only two or three threads.  Furthermore, most
real-world concurrent programs don't have every single thread
operating on the same bit of shared state.  So in practice, we will
tend not to see this exponential growth in schedules tried.

\section{Executing Concurrent Programs}
\label{sec:dejafu-execution}

We represent operations in the \verb|MonadConc| typeclass by the
\verb|Action| type of \emph{primitive actions}, where each action
describes some effect.  Each primitive action also contains a
continuation.  A concurrent computation is just a sequence of these
continuation values.  Each thread of execution is terminated by a
distinguished \emph{stop} primitive, which has no continuation and
signals successful completion.

\begin{listing}
\centering
\begin{cminted}{haskell}
newtype M n r a = M { runM :: (a -> Action n r) -> Action n r }

instance Functor (M n r) where
  fmap f m = M (\c -> runM m (c . f))

instance Applicative (M n r) where
  pure x  = M (\c -> AReturn (c x))
  f <*> v = M (\c -> runM f (\g -> runM v (c . g)))

instance Monad (M n r) where
  return  = pure
  m >>= k = M (\c -> runM m (\x -> runM (k x) c))

instance MonadFail (M n r) where
  fail e = M (\_ -> AThrow (MonadFailException e))
\end{cminted}
\caption{The \dejafu{} continuation monad.}\label{lst:m}
\end{listing}

\Cref{lst:m} gives the definition and typeclass instances of the
\dejafu{} continuation monad.  The \verb|Functor| instance allows
applying a function to the input of the continuation.  The
\verb|Applicative| instance allows injecting a pure value into the
\verb|M| type, by constructing a continuation which consumes this
value.  It also allows extracting a function from one computation, a
value from another, and applying them.  The \verb|Monad| instance
allows sequencing.  Finally, the \verb|MonadFail| instance alows
signalling a pattern match failure in a monadic
expression\footnote{\dejafu{} aims to support the latest three major
  releases of GHC, and so in the real implementation we achieve this
  with conditional compilation.}.

\paragraph{Operational semantics}
We give an operational semantics for Haskell concurrency in the form of a step
function on primitive actions.  Given the current state, which we call the
\emph{context}, the identifier of the chosen thread, and its primitive action,
we either indicate a failure condition, or produce a new state; in both cases we
return a log of what happened, to put into the execution trace returned to the
user.

Our context value does not contain a heap, instead we use Haskell
mutable references directly.  This means that executing our step
function has effects, and in general contexts should not be re-used.
This is not a limitation in practice because we never want to re-use
contexts.  We could include the heap state in our context, by using a
heterogeneous map type, however the additional indirection both
increases allocation, and reduces the efficacy of garbage collection.
By using Haskell mutable references directly, when all copies of a
reference fall out of scope, the referenced value can be garbage
collected; whereas if we model references as keys into a heap, then
such data can never be freed, as we cannot tell when it is safe to
delete something from our heap.

\begin{listing}
\centering
\begin{cminted}{haskell}
pure id <*> v
  = M (\c -> AReturn (c id)) <*> v
  = M (\c -> runM (M (\c -> AReturn (c id))) (\g -> runM v (c . g)))
  = M (\c -> (\c -> AReturn (c id)) (\g -> runM v (c . g)))
  = M (\c -> AReturn ((\g -> runM v (c . g)) id))
  = M (\c -> AReturn (runM v (c . id)))
  = M (\c -> AReturn (runM v c))
 /= v
\end{cminted}
\caption{Expansion of the \texttt{Applicative} identity law.}\label{lst:areturn}
\end{listing}

\paragraph{Nonterminating executions}
\dejafu{} is only able to make scheduling decisions at the level of
primitive actions, which means that if evaluating a primitive action
does not terminate, \dejafu{} will hang.  As \Cref{lst:areturn} shows,
we deliberately break the \verb|Applicative| identity law, that
\verb|pure id <*> v = v| for all \verb|v|, to make some programs more
defined than they otherwise would be.

\begin{listing}
\centering
\begin{cminted}{haskell}
test = forever (pure "loop") where
  forever mx = mx >> forever mx
\end{cminted}
\caption{A simple non-terminating program.}\label{lst:forever}
\end{listing}

Consider \Cref{lst:forever}.  Neither \verb|(>>)| nor \verb|forever|
correspond to primitive actions, so they cannot be pre-empted.  If
\verb|pure| did not correspond to a primitive action either, then that
expression would cause \dejafu{} to loop forever, trying to find the
continuation.  This is an unhelpful result.  By breaking the laws and
introducing a way to interrupt the \verb|forever| computation,
\dejafu{} can instead report that trying to test this program exceeds
the execution length
limit\footnote{\url{https://github.com/barrucadu/dejafu/issues/27} and \texttt{issues/113}}.

\paragraph{Scheduling}
The choice of which thread to execute is made by a scheduler function,
which is supplied by the user.  The scheduler is called even if there
is only one runnable thread, to keep things simple.  A scheduler, has
the type declared in \Cref{lst:scheduler}.  It is a stateful function
which is given the previous action and the runnable threads, which
possibly returns a thread to run.  If no thread is returned, the
computation is aborted.  Aborting is used in the implementation of
schedule bounding.  The state is used in the implementation of
partial-order reduction and random scheduling: in the former, the
state is a list of scheduling decisions intended to put the system
into a new state; in the latter, the state is a random number
generator.

\begin{listing}
\centering
\begin{cminted}{haskell}
newtype Scheduler state = Scheduler
  { scheduleThread
    :: Maybe (ThreadId, ThreadAction)
    -> NonEmpty (ThreadId, Lookahead)
    -> state
    -> (Maybe ThreadId, state)
  }
\end{cminted}
\caption{The \dejafu{} \texttt{Scheduler} type.}\label{lst:scheduler}
\end{listing}

\paragraph{Success and failure}
When testing concurrent computations, we are interested in both
success and failure.  If a computation succeeds and returns a value,
we want to know that; if it fails and deadlocks, we also want to know
that.  The result of a single execution of the program is a value of
type \verb|Either Failure result|, where the \verb|Failure| type is an
enumeration of error conditions that \dejafu{} can detect, and the
\verb|result| type is the result in the successful case.

\subsection{Software Transactional Memory}

Transactions allow the atomic execution of a sequence of operations
involving \verb|TVar|s, transactional variables.  Unlike operations on
\verb|CRef|s or \verb|MVar|s, transactions are composable and the
whole remains atomic.  We express transactions in a similar way to
concurrent programs: as a continuation monad over a primitive action
type.  A complete listing of actions is available in
\Cref{app:primops}.  As \dejafu{} drives the execution of a concurrent
program, it is possible to have arbitrarily complex effects which
appear atomic to the program under test.  We ensure atomicity by
executing primitive actions sequentially.

The STM primitive actions also have an operational semantics defined
by a step function.  Like the small-step semantics of the concurrency
primitive actions, this does not include a model of the heap.
However, as transactions are atomic, we only care about the big-step
behaviour.  This simplifies some aspects of the implementation, but
means that we cannot currently cope with nonterminating transactions,
whereas we can and do abort concurrent programs.

A transaction evaluates to some success value, a thrown exception, or
an abort.  If successful, it may also mutate some transactional
variables as a effect; otherwise it does not.  If it evaluates to a
thrown exception, we throw the exception in the thread performing the
transaction.  If it evaluates to an abort, we block the thread
performing the transaction until at least one \verb|TVar| read in the
transaction is mutated by another thread.

\subsection{Relaxed Memory}

There are three memory models supported in \dejafu{}:

\begin{description}
\item[Sequential Consistency] This model is the most intuitive.  A program
  behaves as a simple interleaving of the actions in different threads.  When a
  \texttt{CRef} is written to, that write is immediately visible to all
  threads.

\item[Total Store Order (TSO)] Each thread has a write buffer.  A thread sees
  its writes immediately, but other threads will only see writes when they are
  \emph{committed}, which may happen later.  Writes by the same thread are
  committed in program order.

\item[Partial Store Order (PSO)] A relaxation of TSO where each thread has a
  write buffer for each \verb|CRef|.  Writes to different \verb|CRef|s by the
  same thread are not necessarily committed in program order.
\end{description}

The default memory model for testing is TSO, as that most accurately
models the behaviour of modern x86 processors\cite{owens2009}.  The
use of a relaxed memory model can require a much larger number of
schedules when \verb|CRef|s are shared between threads.

\paragraph{Write buffering}
We model relaxed memory by introducing buffers for thread writes.  When a thread
writes to a \verb|CRef|, the write is appended to its buffer.  When a thread
reads from a \verb|CRef|, it reads the value of the newest write in its buffer,
or the most recently committed value if the buffer is empty.  After a write is
committed, it is removed from its buffer.  Any non-empty buffer may have a write
committed, but only the oldest write in a buffer may be committed.  \Cref{fig:wb}
shows the arrangement of buffers for the three memory models in a system with
two threads and two \verb|CRef|s.

\begin{figure}
  \centering
  \begin{subfigure}{0.3\textwidth}
    \centering
    \begin{tikzpicture}
      \node (t1) at (-1,0) {$T_1$};
      \node (t2) at (1,0)  {$T_2$};
      \node (v) [draw,shape=circle] at (-1,-4) {$v$};
      \node (w) [draw,shape=circle] at (1,-4)  {$w$};

      \draw [->, thick] (t1.south) to [out=270,in=90] (v.north);
      \draw [->, thick] (t1.south) to [out=270,in=90] (w.north);
      \draw [->, thick] (t2.south) to [out=270,in=90] (v.north);
      \draw [->, thick] (t2.south) to [out=270,in=90] (w.north);
    \end{tikzpicture}
    \caption{Sequential Consistency}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \begin{tikzpicture}
      \node (t1) at (-1,0) {$T_1$};
      \node (t2) at (1,0)  {$T_2$};
      \node (v) [draw,shape=circle] at (-1,-4) {$v$};
      \node (w) [draw,shape=circle] at (1,-4)  {$w$};

      \node (bt1) [draw] at (-1,-1.8) {\footnotesize\begin{tabular}{c} \ldots \\\midrule $w$ \\\midrule $v$ \end{tabular}};
      \node (bt2) [draw] at (1,-1.8) {\footnotesize\begin{tabular}{c} \ldots \\\midrule $w$ \\\midrule $v$ \end{tabular}};

      \draw [->, thick] (t1) to [out=270,in=90] (bt1);
      \draw [->, thick] (t1) to [out=270,in=90] (bt1);
      \draw [->, thick] (t2) to [out=270,in=90] (bt2);
      \draw [->, thick] (t2) to [out=270,in=90] (bt2);

      \draw [->, thick] (bt1.south) to [out=270,in=90] (v);
      \draw [->, thick] (bt1.south) to [out=315,in=90] (w);
      \draw [->, thick] (bt2.south) to [out=225,in=90] (v);
      \draw [->, thick] (bt2.south) to [out=270,in=90] (w);
    \end{tikzpicture}
    \caption{Total Store Order}
  \end{subfigure}
  \begin{subfigure}{0.3\textwidth}
    \centering
    \begin{tikzpicture}
      \node (t1) at (-1,0) {$T_1$};
      \node (t2) at (1,0)  {$T_2$};
      \node (v) [draw,shape=circle] at (-1,-4) {$v$};
      \node (w) [draw,shape=circle] at (1,-4)  {$w$};

      \node (bt1v) [draw] at (-1.5,-1.8) {\footnotesize\begin{tabular}{c} \ldots \\\midrule $v$ \\\midrule $v$ \end{tabular}};
      \node (bt1w) [draw] at (-0.5,-1.8) {\footnotesize\begin{tabular}{c} \ldots \\\midrule $w$ \\\midrule $w$ \end{tabular}};
      \node (bt2v) [draw] at (0.5,-1.8) {\footnotesize\begin{tabular}{c} \ldots \\\midrule $v$ \\\midrule $v$ \end{tabular}};
      \node (bt2w) [draw] at (1.5,-1.8) {\footnotesize\begin{tabular}{c} \ldots \\\midrule $w$ \\\midrule $w$ \end{tabular}};

      \draw [->, thick] (t1) to [out=270,in=90] (bt1v);
      \draw [->, thick] (t1) to [out=270,in=90] (bt1w);
      \draw [->, thick] (t2) to [out=270,in=90] (bt2v);
      \draw [->, thick] (t2) to [out=270,in=90] (bt2w);

      \draw [->, thick] (bt1v.south) to [out=270,in=90] (v);
      \draw [->, thick] (bt1w.south) to [out=270,in=90] (w);
      \draw [->, thick] (bt2v.south) to [out=270,in=90] (v);
      \draw [->, thick] (bt2w.south) to [out=270,in=90] (w);
    \end{tikzpicture}
    \caption{Partial Store Order}
  \end{subfigure}
  \caption{Example of write buffering for two threads and two \texttt{CRef}s.}
  \label{fig:wb}
\end{figure}

We divide operations into three categories: \emph{synchronised}
operations impose a \emph{memory barrier}, committing all writes;
\emph{partially synchronised} operations commit one or more writes to
the same \verb|CRef|; and \emph{unsynchronised} operations never cause
a commit.

\paragraph{Phantom threads}
In a sequentially consistent memory model, the set of runnable threads is
exactly the set of threads created by forking which are not blocked.  Under
relaxed memory, however, this is not the case.  For each write buffer, we
introduce one \emph{phantom thread}.  When scheduled, a phantom thread
commits the oldest write in its corresponding buffer.

This may seem like an odd approach: why create new threads to model relaxed
memory?  The advantage is that systematic concurrency testing techniques assume
there is only one source of nondeterminism: the scheduler.  If a second source
is added, such as when writes are committed, it is difficult to integrate this
with existing algorithms.  By using phantom threads, the two sources of
nondeterminism are unified, and existing algorithms just work.  We take this
approach from \cite{zhang2015}.

\section{Testing Concurrent Programs}
\label{sec:dejafu-testing}

\dejafu{} uses a combination of dynamic partial-order reduction and
schedule bounding to test programs, by default.  Controlled random
scheduling using a fixed number of executions is also available.  A
\dejafu{} test has the following components:

\begin{itemize}
\item The testing algorithm to use plus any configuration it needs.
  The default is DPOR with schedule bounding.
\item The memory model.  The default is total store order (TSO), as it
  is closest to the behaviour of an x86 processor\cite{owens2009},
  which is what the user probably has.
\item A name.
\item A predicate, which is a function from the final list of results
  and traces to an indication of success or failure with an optional
  list of failing traces to display to the user.
\item A function to optionally discard results as they are produced,
  not considering them when determining if the test passes.  This is
  for performance: execution traces can use a lot of memory, and
  typically we are only interested in a subset.
\item Finally, the \verb|MonadConc| action to test.
\end{itemize}

To make the tool easier to use, we provide a collection of different
testing functions, with varying levels of detail exposed, hoping that
the defaults will suffice for most users.

\paragraph{Dependency relation}
Correct DPOR relies on a dependency relation between pairs of actions
in a schedule trace.  This relation may have false positives, but
cannot have false negatives.  False positives may lead to exploring
redundant executions, false negatives may lead to missing distinct
ones.

For ease of explanation, DPOR algorithms in the literature are
presented for small languages.  A paper will typically start with a
sentence like ``we assume a core concurrent language of reads and
writes to shared variables, and locks.''  For example, in
\cite{coons2013} two actions are said to be dependent if (1) they are
actions of the same thread, or (2) they are both actions on the same
shared variable and at least one is a write.  The Haskell concurrency
API is richer than this, and the implicit dependencies between actions
(such as which actions impose a memory barrier) are not documented.
The dependency relation we now use in \dejafu{} was derived in part by
running small test cases.

We express our dependency relation in three parts: (1) a relation on
\emph{thread actions}, which are entries in the trace; (2) a relation
between \emph{thread action} and \emph{lookahead values}, which are
forecasts of what a thread will do, available to the scheduler; (3) a
relation on \emph{action types}, a simplification of thread actions.
Thread actions contain more information than lookahead values, which
contain more information than action types.  We use this layered
approach to make use of this extra information to improve dependency
detection where possible.  Furthermore, our dependency relation is
\emph{conditional}\cite{godefroid1993}, we record which \verb|CRef|s
have buffered writes and what the masking state of every thread is,
allowing more precise decisions.  \Cref{lst:deprel-simp} shows a how
the relations fit together.  For our overall dependency relation to be
correct, we require some consistency rules:

\begin{listing}
\centering
\begin{cminted}{haskell}
dependent :: (ThreadId, ThreadAction) -> (ThreadId, ThreadAction) -> Bool
dependent (t1, a1) (t2, a1)
  | ...
  | otherwise = dependent' (t1, a1) (t2, rewind a2) &&
                dependent' (t2, a2) (t1, rewind a1)

dependent' :: (ThreadId, ThreadAction) -> (ThreadId, Lookahead) -> Bool
dependent' (t1, a1) (t2, l2)
  | ...
  | otherwise = dependentActions (simplifyAction    a1)
                                 (simplifyLookahead l2)

dependentActions :: ActionType -> ActionType -> Bool
\end{cminted}
\caption{A simplified view of the \dejafu{} dependency relations.}\label{lst:deprel-simp}
\end{listing}

\begin{description}
\item[Commutativity (1)]
  \verb|dependent = flip dependent|
\item[Commutativity (2)]
  \verb|dependentActions = flip dependentActions|
\item[Consistency (1)]
  \verb|dependent x (t, a) ==> dependent' x (t, rewind a)|
\item[Consistency (2)]
  \verb|simplifyLookahead = simplifyAction . rewind|
\end{description}

We establish these conditions through the syntactic structure of the
implementation.  Each function is a straightforward enumeration of
cases with attached side-conditions, which makes commutativity
apparent.  The more pessimistic \verb|dependent'| function is similar
to the more accurate \verb|dependent| function, but has less
information available, which makes consistency apparent.  We also
include these properties in the \dejafu{} test suite to ensure they
are not inadvertently broken.  \Cref{app:deprel} defines the
dependency relation in full.

Through the lens of \verb|dependentActions|, Haskell is a small
concurrent language with synchronised and unsynchronised shared
variables and memory barriers.  The other cases which are handled by
\verb|dependent| and \verb|dependent'| are: lifted \verb|IO| actions,
exceptions, STM transactions, and getting and setting the number of
capabilities.

\paragraph{Sleep sets}
The sleep set optimisation\cite{godefroid1996} is a complementary
approach to DPOR which we use to further reduce schedules explored.
The intuition is as follows: if we have a choice of two scheduling
decisions, $t_{1}$ and $t_{2}$, after trying out $t_{1}$ there is no
point in making the sequence of decisions $t_{2}t_{1}$ unless
$t_{1} \dependent t_{2}$.  This is because all states reachable from
$t_{1}$ have already been explored, so the only way a new state could
arise is if $t_{1}$ had a different effect, and that is what it means
for two transitions to be dependent.


\begin{figure}
  \centering
  \begin{tikzpicture}[ball/.style = {circle, draw, align=center, anchor=north, inner sep=0, text width=0.5cm}]
    \node[ball]              (A) at (0,0)   {\small $s_0$};
    \node[ball]              (B) at (-1,-1) {};
    \node[ball]              (C) at (1,-1)  {\small $s_1$};
    \node[ball, opacity=0.4] (D) at (0,-2)  {};
    \node[ball]              (E) at (2,-2)  {\small $s_2$};
    \node[ball]              (F) at (1,-3)  {};

    \node (init) at (0,0.75)   {};
    \node (end1) at (-1,-2.25) {};
    \node (end2) at (0,-3.25)  {};
    \node (end3) at (3,-3)     {};
    \node (end4) at (1,-4.25)  {};
    \draw[->, thick, draw, dashed] (init.south) to [out=270,in=90] (A.north);
    \draw[->, thick, draw, dashed] (B.south) to [out=270,in=90] (end1.north);
    \draw[->, thick, draw, dashed] (D.south) to [out=270,in=90] (end2.north);
    \draw[->, thick, draw, dashed] (E.south) to [out=270,in=90] (end3.north);
    \draw[->, thick, draw, dashed] (F.south) to [out=270,in=90] (end4.north);

    \draw[->, thick, draw]              (A.south) to [out=270,in=90] node[midway,above] {\small $t_1$} (B.north);
    \draw[->, thick, draw]              (A.south) to [out=270,in=90] node[midway,above] {\small $t_2$} (C.north);
    \draw[->, thick, draw, opacity=0.4] (C.south) to [out=270,in=90] node[midway,above] {\small $t_1$} (D.north);
    \draw[->, thick, draw]              (C.south) to [out=270,in=90] node[midway,above] {\small $t_2$} (E.north);
    \draw[->, thick, draw]              (E.south) to [out=270,in=90] node[midway,above] {\small $t_1$} (F.north);

    \node (note) at (0,-5) {
      \small \textbf{Dependencies:} \hspace{0.1cm} $t_1 \independent t_2$ \hspace{0.1cm} $t_2 \dependent t_3$
    };
  \end{tikzpicture}
  \caption[The sleep set optimisation.]{The sleep set optimisation.  Transition $t_1$ may be pruned in state $s_1$ but not in state $s_2$.  The transition has been explored from state $s_0$ and there is no dependent transition between states $s_0$ and $s_1$, but there is between states $s_0$ and $s_2$.  Adapted from~\cite{coons2013}.}\label{fig:sleep}
\end{figure}

Formally, we augment each state $s$ with a \emph{sleep set},
containing transitions enabled in $s$ but which we will not make.  The
initial state has an empty sleep set.  Let $T$ be the transitions that
have been selected to be explored from $s$.  We proceed as follows:
take a transition $t_{1}$ out of $T$.  The sleep set associated with
the state reached after executing $t_{1}$ from $s$ is the sleep set
associated with $s$, minus all transitions that are dependent with
$t_{1}$.  Let $t_{2}$ be a second transition taken out of $T$.  The
sleep set associated with the state reached after executing $t_{2}$
from $s$ is the sleep set associated with $s$ augmented with $t_{1}$,
minus all transitions that are dependent with $t_{2}$.  We continue
until all transitions in $T$ have been explored, at each step adding
the previously taken transitions to the sleep set of the new state,
and removing the dependent transitions.

\paragraph{Schedule bounding}
\dejafu{} supports pre-emption bounding\cite{musuvathi2007}; fair
bounding\cite{musuvathi2008}; and depth, or length,
bounding\cite{russell2002}.  Fair and length bounding allow the
testing of cyclic or potentially infinite state-spaces, at the cost of
potentially confusing the user by aborting the execution of their
program when it is still runnable.  All three bounds are enabled by
default, but can be selectively disabled or customised.

\paragraph{Daemon threads}
A daemon thread is a thread which is automatically killed after the
last non-daemon thread terminates.  Daemon threads do not prevent
termination of the program.  In Haskell, every thread other than the
main thread is a daemon thread, so as soon as the main thread
terminates the whole program terminates.  This is a problem for DPOR,
as it makes the last action of the execution depend on everything else
in the program!

\begin{listing}
\centering
\begin{cminted}{haskell}
main = do
  v <- newEmptyMVar
  fork (myThreadId >> putMVar v "hello world")
  tryReadMVar v
\end{cminted}
\caption{A program with a race condition.}\label{lst:daemon1}
\end{listing}

\Cref{lst:daemon1} has two possible results: \verb|Nothing|, and
\verb|Just "hello world"|.  We want like \dejafu{} to produce both of
these.  There is no dependency between \verb|myThreadId| and
\verb|tryReadMVar|, so if the deterministic scheduler used favours the
main thread, we may not see the second execution.  Introducing a
dependency between the last action of the execution and everything
else solves this problem.

\begin{listing}
\centering
\begin{cminted}{haskell}
main = do
  v <- newEmptyMVar
  fork (myThreadId >> myThreadId >> putMVar v "hello world")
  tryReadMVar v
\end{cminted}
\caption{Another program with a race condition.}\label{lst:daemon2}
\end{listing}

However, these new dependencies also present a difficulty.  In
\Cref{lst:daemon2}, the forked thread now performs \verb|myThreadId|
twice.  If the \verb|tryReadMVar| happens after the second of these,
we get the same result as if it happens after the first.  Normally,
DPOR would recognise this and prune the redundant decision, as
\verb|myThreadId| and \verb|tryReadMVar| are independent.  However, by
introducing a dependency between the final action and everything else,
we have told the algorithm that it is \emph{not} redundant!  In
general, introducing a dependency like this will lead to many
redundant executions which we would otherwise avoid.

The solution we adopt is to change the deterministic scheduler.  If
the scheduler has a choice of actions, where one or more will cause
the main thread to terminate, it makes its choice as usual, and then
records every other decision as a backtracking point.  By ensuring
that every other decision is tried at least once, we do not need to
introduce an additional dependency between the final action of the
main thread and everything else, and can just let DPOR do its job as
usual.

\section{Soundness and Completeness}
\label{sec:dejafu-correctness}

Correctness for \dejafu{} breaks down along a natural separation in
the theory: correctness of a single execution of a concurrent program,
and correctness of the testing framework around this which discovers
new executions.  We can state correctness as `nothing is missed and
nothing is made up,' or:

\begin{description}
\item[Soundness] \dejafu{} only reports results of the program which
  could arise under normal execution.
\item[Compleness] \dejafu{} reports all results of the program which
  could arise under normal execution, taking any schedule bounds into
  account.
\end{description}

There is also the concern of correctness of the implementation with
respect to the theory.  We do not attempt to formally verify the
implementation, but we do have an extensive test suite.  Testing a
testing tool is a little different to testing a regular program.  Most
of our tests consist of \emph{nondeterministic} programs, where we
want to verify that \dejafu{} finds precisely the behaviours we
expect.

\paragraph{Property tests}
\dejafu{} has quite a few small functions which are used extensively,
and which have some invariants relating them.  For example, the three
functions making up the dependency relation in
\Cref{sec:dejafu-testing}.  We use LeanCheck\cite{leancheck} to check
these invariants.

\paragraph{Integration tests}
Most of the tests are integration tests, consisting of a small
concurrent program and a property to check.  A failure in such an
integration test could in principle be anywhere in \dejafu{}, and so
hard to isolate and fix, but in practice failures in different
components tend to manifest differently.  Our integration tests fall
into the following classes:

\begin{itemize}
\item Single threaded tests for the \verb|MonadConc| primitives,
  ensuring that only the single correct behaviour is observed.
\item Multi-threaded tests for the \verb|MonadConc| primitives,
  ensuring that the expected nondeterminism is observed.
\item So-called \emph{litmus tests} for the relaxed memory
  implementation, ensuring that all expected relaxed behaviours are
  observed.
\item Tests which selectively discard results.
\item A copy of the async\cite{async} test suite, for our
  \verb|MonadConc| version.
\item A collection of refinement tests from CoCo, which we shall
  discuss in \Cref{chp:coco}.
\item Finally, a collection of regression tests for bugs which have
  been fixed.
\end{itemize}

\paragraph{Example programs}
Finally, we have a collection of larger integration tests which serve
also as example usages of \dejafu{}: the auto-update\cite{auto_update}
library, which we discuss in \Cref{sec:dejafu-casestudies}; the Dining
Philosophers problem; a message queue; and an example of testing
typeclass laws using QuickCheck\cite{claessen2000} and concurrency.

\subsection{Correct Execution}

Correctness of execution asks: can the result of an arbitrary
execution of \dejafu{}'s testing implementation can be obtained in
reality?  Furthermore, do all real-world executions correspond to a
possible execution under \dejafu{}?  Both of these come with the
caveat that the behaviour can be different, as long as this difference
can't be observed.

\paragraph{Program behaviour}
There is no standard for concurrent Haskell.  There is only what GHC
provides.  The behaviour of many operations is clear, but for some it
is not.  \verb|CRef| operations are particularly complicated, as their
behaviour depends on the underlying memory model, which is
unspecified.  We chose TSO, but ignore the possibility that the GHC
optimiser or code generator could affect the memory model.

There are some intentional semantic differences for practical reasons.
For example, GHC can sometimes detect deadlocks involving only a
subset of the threads, and throw an exception to the threads
signalling this.  We cannot do this.

Although the behaviour of \dejafu{} is not correct with respect to
GHC-compiled behaviour in all cases, but we claim it is as close as
can reasonably be achieved.

\paragraph{Possible executions}
Our stepwise execution of concurrent programs allows a scheduling
decision to be made between each primitive action, which doesn't
correspond to how GHC handles scheduling:

\begin{displayquote}
  GHC implements pre-emptive multitasking: the execution of threads
  are interleaved in a random fashion.  More specifically, a thread may
  be pre-empted whenever it allocates some memory, which unfortunately
  means that tight loops which do no allocation tend to lock out other
  threads (this only seems to happen with pathological benchmark-style
  code, however).\cite{control_concurrent}
\end{displayquote}

So there are executions involving the pre-emption of the evaluation of
non-terminating expressions which are possible under GHC but not under
\dejafu{}.  Whether this can be used to produce different outputs is
unclear.

\subsection{Correct Testing}

Correctness of testing asks: are the schedule prefixes generated by
the DPOR machinery are valid?  Furthermore, are there any possible
results for which no schedule will be generated?  This is different to
the testing framework generating every schedule, as that is precisely
what DPOR tries to avoid.

\paragraph{Prefix validity}
Executions are stored internally as a stack, shown in \Cref{lst:dpor}.
The sequence of thread IDs corresponding to this stack represents a
complete execution of the program.  There is a unique initial state,
where only the initial thread is runnable and nothing has been done.

\begin{listing}
\centering
\begin{cminted}{haskell}
data DPOR = DPOR
  { dporRunnable :: Set ThreadId
  -- ^ What threads are runnable at this step.
  , dporTodo     :: Map ThreadId Bool
  -- ^ Follow-on decisions still to make, and whether that decision
  -- was added conservatively due to the bound.
  , dporNext     :: Maybe (ThreadId, DPOR)
  -- ^ The next decision made. Executions are explored in a
  -- depth-first fashion, so this changes as old subtrees are
  -- exhausted and new ones explored.
  , dporDone     :: Set ThreadId
  -- ^ All transitions which have been taken from this point,
  -- including conservatively-added ones.
  , dporSleep    :: Map ThreadId ThreadAction
  -- ^ Transitions to ignore (in this node and children) until a
  -- dependent transition happens.
  , dporTaken    :: Map ThreadId ThreadAction
  -- ^ Transitions which have been taken, excluding
  -- conservatively-added ones. This is used in implementing sleep
  -- sets.
  } deriving (Eq, Show)
\end{cminted}
\caption[The \dejafu{} DPOR state.]{The DPOR state is a stack of scheduling decisions.}\label{lst:dpor}
\end{listing}

There are some basic well-formedness properties associated with a
\verb|DPOR| value: (1) every thread in the to-do set is runnable; (2)
every thread in the done set is runnable; (3) the taken set is a
subset of the done set; (4) the done and to-do sets are disjoint; (5)
the done and sleep sets are disjoint; (6) the next-taken thread, if
there is one, is in the done set; and (7) these properties hold
recursively.

A generated schedule prefix is a sequence of taken decisions followed
by a single to-do decision.  By maximising the length of prefixes, we
obtain a depth-first search of the space of schedules.  As long as the
well-formedness properties hold, and the runnable sets are correctly
recorded during execution, then a generated schedule prefix will be
valid.

\paragraph{Schedule completeness}
The DPOR machinery should eventually find every possible result of a
given program.  However, as schedule bounding is involved, some
results may not be reached.  So we have two slightly different notions
of completeness:

\begin{enumerate}
\item When all schedule bounds are disabled, all possible results show
  up under testing.
\item For all sets of bounds, all results possible subject to those
  bounds show up under testing with the same bounds.
\end{enumerate}

Property (2) implies property (1).  We use \cite{coons2013} as our
core DPOR algorithm, which establishes property (2).

\section{Case Studies}
\label{sec:dejafu-casestudies}

We now discuss three case studies, demonstrating typical uses of
\dejafu{}:

\begin{enumerate}
\item We reproduce a bug in the auto-update\cite{auto_update} library.
\item We test one of the Par monad\cite{marlow2011} schedulers.
\item Finally, we use property-based testing to reproduce a bug in the
  async\cite{async} library.
\end{enumerate}

Each case study involved porting existing code to the \verb|MonadConc|
abstraction.

\subsection{The auto-update Package}

The auto-update library\cite{auto_update} runs tasks periodically, but
only if needed.  For example, a web server may handle each request in
a new thread, and log the time that the request arrives.  Rather than
have every such new thread check the time, one thread could be created
to update a single shared \verb|CRef| every second.  However, if the
request frequency is less than once per second, this is wasted work.
The auto-update library allows defining an action with a frequency,
which will be executed appropriately, regardless of how busy the
system is.

The entire implementation, excluding comments and imports, is
reproduced at the end of the subsection in \Cref{lst:autoupdate}.  The
library defines a function, \verb|mkAutoUpdate|, which forks a worker
thread to perform the action at a frequency, but only if the value is
required.  The function returns an \verb|IO| action to read the
current result, if necessary blocking until there is one.  The
transformation to the \verb|MonadConc| typeclass is straightforward,
and we omit it here.

\begin{listing}
\centering
\begin{cminted}{haskell}
test_autoupdate :: MonadConc m => m ()
test_autoupdate = do
  auto <- mkAutoUpdate defaultUpdateSettings
  auto
\end{cminted}
\caption{An example usage of the auto-update library.}\label{lst:autoupdate_example1}
\end{listing}

\Cref{lst:autoupdate_example1} shows an example usage of the
\verb|MonadConc| version of the library.  The
\verb|defaultUpdateSettings| value describes an auto-updater which
runs every second, producing the value \verb|()| (read ``unit'').
Internally, an \verb|IORef| is created to hold the most recent value,
and a thread created to update this value.  An \verb|MVar| is used to
communicate to the thread that the updater should run: this is what
gives the rate limiting.  So we create an auto-updater which produces
\verb|()|, and immediately demand the value.

\begin{listing}
\centering
\begin{cminted}{text}
> autocheck test_autoupdate

[fail] Never Deadlocks
        [deadlock] S0--------S1-----------S0-
[pass] No Exceptions
[fail] Consistent Result
        () S0--------S1--------p0--

        [deadlock] S0--------S1-----------S0-
\end{cminted}
\caption[Using \dejafu{} to run a collection of standard tests.]{Using \dejafu{} to run a collection of standard tests.  The \texttt{autocheck} function looks for deadlocks, uncaught exceptions in the main thread, and nondeterminism.  Each result is displayed with a simplified view of a representative execution trace.}\label{lst:autoupdate_example2}
\end{listing}

\Cref{lst:autoupdate_example2} shows one way in which we can use
\dejafu{} to explore the behaviour of our small example.  The
\verb|autocheck| function looks for some common sources of error:
deadlocks, uncaught exceptions in the main thread, and nondeterminism.
In this example, \dejafu{} discovers a deadlock.

Each result is displayed with a simplified view of a representative
execution trace:

\begin{itemize}
\item \verb|Sn|: indicates that thread \verb|n| started executing
  after the previously executing thread blocked or terminated.
  Threads are numbered sequentially from zero as they are created, and
  may be given names.
\item \verb|Pn|: indicates that thread \verb|n| started executing by
  pre-empting the previously executing thread.
\item \verb|pn|: indicates that thread \verb|n| started executing
  after the previously executing thread yielded or delayed.
\item \verb|-|: indicates the execution of one primitive action.
\end{itemize}

More detailed execution traces are also available, which contain a
summary of the primitive actions which occurred and the alternative
scheduling decisions available.

So, now knowing what the traces represent, we can decipher the output
in \Cref{lst:autoupdate_example2}.  The deadlock is the more
interesting case, as it is hopefully unintentional, so let's look at
that one.  We can see from the trace that thread 0 executed for a
while, then thread 1, then thread 0 again.  As these are all \verb|S|
points, each thread executed until it blocked.  So we can look at the
source code in \Cref{lst:autoupdate_example1} and
\Cref{lst:autoupdate} to see what happened.

Following the execution by eye, we see the following sequence of
concurrent events:

\begin{enumerate}
\item Thread 0:
  \begin{enumerate}
  \item Line 16: \verb|currRef <- newIORef Nothing|
  \item Line 17: \verb|needsRunning <- newEmptyMVar|
  \item Line 18: \verb|lastValue <- newEmptyMVar|
  \item Line 20: \verb|void $ forkIO $ ...|
  \item Line 35: \verb|mval <- readIORef currRef|
  \item Line 39: \verb|void $ tryPutMVar needsRunning ()|
  \item Line 40: \verb|readMVar lastValue|
  \item \textbf{Thread 0 is now blocked, as \texttt{lastValue} is empty.}
  \end{enumerate}
\item Thread 1:
  \begin{enumerate}
  \item Line 21: \verb|takeMVar needsRunning|
  \item Line 25: \verb|writeIORef currRef $ Just a|
  \item Line 26: \verb|void $ tryTakeMVar lastValue|
  \item Line 27: \verb|putMVar lastValue a|
  \item \textbf{Thread 0 is now unblocked, as \texttt{lastValue} is full.}
  \item Line 29: \verb|threadDelay $ updateFreq us|
  \item Line 31: \verb|writeIORef currRef Nothing|
  \item Line 32: \verb|void $ takeMVar lastValue|
  \item \textbf{Thread 0 is still unblocked, even though \texttt{lastValue} is now empty again.}
  \item \textbf{Thread 1 now loops.}
  \item Line 21: \verb|takeMVar needsRunning|
  \item \textbf{Thread 1 is now blocked, as \texttt{needsRunning} is empty.}
  \end{enumerate}
\item Thread 0:
  \begin{enumerate}
  \item Line 40: \verb|readMVar lastValue|
  \item \textbf{Thread 0 is now blocked, as \texttt{lastValue} is empty.}
  \end{enumerate}
\end{enumerate}

Both threads are blocked, so the computation is deadlocked.  The other
result shown in \Cref{lst:autoupdate_example2} occurs if thread 0
starts executing after thread 1 delays.  So the root cause of this
deadlock is clear: deadlock may occur if the call to
\verb|threadDelay| on line 29 completes before the other thread
resumes execution.  Despite this bug being rather simple, not
requiring any pre-emptions at all to trigger, it arose in practice.
How easy it is to make mistakes when implementing concurrent programs!

\begin{table}
  \centering
  \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{lSSSS} \toprule
      & {Schedules} & {Deadlocks} & {Time (s)} & {Max Residency (kB)} \\ \midrule
      Bounded DPOR   &  49 & 18 & 0.006 &  119 \\
      Unbounded DPOR &  80 & 20 & 0.008 &  124 \\
      Swarm$\dagger$ & 100 & 20 & 0.008 & 1297 \\ \bottomrule
    \end{tabular}
    \caption{Keeping all execution traces in memory.}\label{tbl:autoupdate_perf1}
  \end{subtable}

  % [layout hack]: no gap between the tables otherwise
  \vspace{2.5em}

  \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{lSSSS} \toprule
      & {Schedules} & {Deadlocks} & {Time (s)} & {Max Residency (kB)} \\ \midrule
      Bounded DPOR   &  49 & 18 & 0.006 &  71 \\
      Unbounded DPOR &  80 & 20 & 0.006 &  63 \\
      Swarm$\dagger$ & 100 & 20 & 0.006 & 108 \\ \bottomrule
    \end{tabular}
    \caption{Only keeping buggy execution traces in memory.}\label{tbl:autoupdate_perf2}
  \end{subtable}
  \caption[Performance of the auto-update case study with three different exploration tactics.]{Performance of the auto-update case study with three different exploration tactics.  Swarm scheduling is a randomised approach discussed in \Cref{chp:algorithms}.}\label{tbl:autoupdate_perf}
\end{table}

\Cref{tbl:autoupdate_perf} shows performance measurements for our test
case in six different configurations: three different algorithms to
explore the space of schedules, keeping or discarding execution
traces.  Both the library itself and our test case are small, so it is
perhaps no surprise to see that in all configurations, execution only
takes a fraction of a second.  We can see the effect of the schedule
bounding: when the bounds are disabled, the number of schedules tried
almost doubles, and two new deadlocking executions are found.

To reduce memory usage, \dejafu{} is able to discard results or
execution traces which the user considers uninteresting in some way.
\Cref{tbl:autoupdate_perf2} shows the impact of this change, where we
have designated non-deadlocking traces as uninteresting.  The effect
is particularly significant in the Swarm case, suggesting that Swarm
may tend to find longer execution traces than DPOR.

\begin{listing}
  \centering
  \begin{minipage}{0.5\textwidth}
  \begin{minted}[linenos]{haskell}
data UpdateSettings a = UpdateSettings
    { updateFreq           :: Int
    , updateSpawnThreshold :: Int
    , updateAction         :: IO a
    }

defaultUpdateSettings :: UpdateSettings ()
defaultUpdateSettings = UpdateSettings
    { updateFreq           = 1000000
    , updateSpawnThreshold = 3
    , updateAction         = return ()
    }

mkAutoUpdate :: UpdateSettings a -> IO (IO a)
mkAutoUpdate us = do
    currRef      <- newIORef Nothing
    needsRunning <- newEmptyMVar
    lastValue    <- newEmptyMVar

    void $ forkIO $ forever $ do
        takeMVar needsRunning

        a <- catchSome $ updateAction us

        writeIORef currRef $ Just a
        void $ tryTakeMVar lastValue
        putMVar lastValue a

        threadDelay $ updateFreq us

        writeIORef currRef Nothing
        void $ takeMVar lastValue

    pure $ do
        mval <- readIORef currRef
        case mval of
            Just val -> return val
            Nothing -> do
                void $ tryPutMVar needsRunning ()
                readMVar lastValue

catchSome :: IO a -> IO a
catchSome act = catch act $
  \e -> pure $ throw (e :: SomeException)
  \end{minted}
  \end{minipage}
  \caption{The implementation of the auto-update package.}\label{lst:autoupdate}
\end{listing}

% [layout hack]: get lst:autoupdate here.
\FloatBarrier

\subsection{The Par Monad}

The Par monad\cite{marlow2011} provides a traditional-looking
concurrency abstraction, giving the programmer threads and mutable
state, however it is deterministic.  Determinism is enforced by
restricting shared state: it is an error to write more than once to
the same variable, and read operations block until a value has been
written.  Programs written using the Par monad will either give a
deterministic result, or terminate with a multiple-write error.  These
shared variables, called \verb|IVar|s, implement
futures\cite{marlow2011}.  Despite its limitations, the library can be
effective in speeding up pure code\cite{marlow2011}.

The library provides six different schedulers.  We ported the
``direct'' scheduler, a work-stealing scheduler, to the
\verb|MonadConc| typeclass.  This was a straightforward and
compiler-driven refactoring.  Changing function types to use
\verb|MonadConc| rather than \verb|IO| led to compiler errors showing
where the next changes needed to be made.  We iterated this process of
fixing errors and recompiling until the library successfully compiled
once more.  Changes were needed in two of the source files.

Some simplifications were made in the conversion process:

\begin{itemize}
\item The scheduler creates a pseudorandom number generator for each
  worker thread.  As systematic testing requires that the scheduler be
  the only source of nondeterminism, we fixed the random seeds: the
  first worker thread gets the seed zero, the second gets the seed
  one, and so on.
\item The scheduler uses the C pre-processor to choose between
  different implementations of some of its functionality.  There are
  nine flags, each of which can be enabled or disabled.  We only
  ported the default configuration.
\item The scheduler includes some debugging code for detecting and
  reporting errors.  We removed it.
\end{itemize}

\begin{table}
  \centering
  \begin{tabular}{lrr} \toprule
    & Direct.hs & DirectInternal.hs \\ \midrule
    Language extensions & 1 & 0 \\
    Module imports & 6 & 1 \\
    Type definitions & 7 & 13 \\
    Type signatures & 32 & 7 \\
    Function renames & 13 & 5 \\
    Logic changes & 16 & 0 \\ \midrule
    \emph{Total} & 75 & 26 \\ \bottomrule
  \end{tabular}
  \caption{Breakdown of changes to port the Par monad ``direct'' scheduler.}\label{tbl:parmonad_diff}
\end{table}

\Cref{lst:parmonad} shows the original and converted versions of the
scheduler initialisation code.  As can be seen, they are similar, even
though this is a core component of a rather sophisticated library,
where the types have been changed.  \Cref{tbl:parmonad_diff} breaks
down the changes across both files.  Code changes are broken down into
``renames,'' where the concurrency library simply provides a different
name for a function, and ``logic,'' where that was not the case.  The
logic changes were either: (1) places where a call to \verb|liftIO| or
\verb|lift| was now necessary; or (2) inserting uses of
\verb|getNumCapabilities|.

\begin{listing}
\centering
\begin{cminted}{haskell}
parfilter :: (MonadConc m, NFData a) => (a -> Bool) -> [a] -> Par m [a]
parfilter _ []  = pure []
parfilter f [x] = pure (if f x then [x] else [])
parfilter f xs  = do
    let (as, bs) = halve xs
    v1 <- Par.spawn (parfilter f as)
    v2 <- Par.spawn (parfilter f bs)
    left  <- Par.get v1
    right <- Par.get v2
    pure (left ++ right)
  where
    halve xs = splitAt (length xs `div` 2) xs
\end{cminted}
\caption{An example usage of the Par monad library.}\label{lst:parmonad_example1}
\end{listing}

\Cref{lst:parmonad_example1} shows an example usage of the library.
This \verb|parfilter| function filters a list in parallel, using a
divide-and-conquer approach.  If the list is not empty or a singleton,
it is split in half and a new thread created to filter each half.  The
results of each new thread are combined to produce the overall result.
The library requires all shared state have an instance of the
\verb|NFData| typeclass, which provides an operation to evaluate data
to normal form.  The Par monad gains its speed by evaluating data in
separate threads.

\begin{listing}
\centering
\begin{cminted}{haskell}
test_parmonad :: (MonadConc m, MonadIO m) => [Int] -> m Bool
test_parmonad xs = do
    let p x = x `mod` 2 == 0
    s <- runPar (parfilter p xs)
    pure (s == filter p xs)
\end{cminted}
\caption{A test case comparing parallel filter to a normal filter.}\label{lst:parmonad_example2}
\end{listing}

\Cref{lst:parmonad_example2} shows a test case for \verb|parfilter|.
A parallel filter should produce the same result as a normal filter.
\Cref{tbl:parmonad_perf} shows performance measurements for our test
case, with the input list \verb|[0..5]|, in six different
configurations.  The numbers do not look so good for DPOR.  Swarm
managed to find two deadlocking executions out of 100, whereas bounded
DPOR found none in 6140.  Unbounded DPOR did not complete at all: it
rapidly consumed all the memory of the host system when keeping traces
in memory, and was still running after two days while discarding
traces.

\begin{table}
  \centering
  \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{lSSSS} \toprule
      & {Schedules} & {Deadlocks} & {Time (s)} & {Max Residency (MB)} \\ \midrule
      Bounded DPOR   & 6140 & 0 & 35.1 &            339 \\
      Unbounded DPOR &      &   &      & {$\geq$ 16GiB} \\
      Swarm          &  100 & 2 & 0.17 &             21 \\ \bottomrule
    \end{tabular}
    \caption{Keeping all execution traces in memory.}\label{tbl:parmonad_perf1}
  \end{subtable}

  % [layout hack]: no gap between the tables otherwise
  \vspace{2.5em}

  \begin{subtable}{\textwidth}
    \centering
    \begin{tabular}{lSSSS} \toprule
      & {Schedules} & {Deadlocks} & {Time (s)} & {Max Residency (kB)} \\ \midrule
      Bounded DPOR   & 6140 & 0 &           27.9 &  842 \\
      Unbounded DPOR &      &   & {$\geq$ 48hrs} &      \\
      Swarm          &  100 & 2 &           0.14 & 1600 \\ \bottomrule
    \end{tabular}
    \caption{Only keeping buggy execution traces in memory.}\label{tbl:parmonad_perf2}
  \end{subtable}
  \caption[Performance of the Par monad case study with three different exploration tactics.]{Performance of the Par monad case study with three different exploration tactics.  Unbounded DPOR was aborted in both cases, after consuming too many resources.}\label{tbl:parmonad_perf}
\end{table}

When we inspect one of the execution traces leading to deadlock, we
gain two clues for why DPOR performs poorly: (1) the trace is 793
entries long, but the length bound for DPOR is 250; and (2) there are
473 calls to \verb|liftIO|, \dejafu{} considers all \verb|IO| actions
dependent and so tries every possible interleaving of those.

\begin{listing}
\centering
\begin{cminted}{text}
(Continue,[],LiftIO)
(Continue,[],LiftIO)
(Continue,[],LiftIO)
(Continue,[],ReadCRef 2)
(Continue,[],LiftIO)
(Continue,[],NewMVar 7)
(Continue,[],ModCRef 4)
(Continue,[],GetNumCapabilities 2)
(Continue,[],BlockedTakeMVar 7)
\end{cminted}
\caption{The final ten entries of the deadlocking Par monad trace.}\label{lst:parmonad_example3}
\end{listing}

Following the code by eye as we read a 793-entry trace is not a
feasible approach here.  So instead, let's look at the last few
entries in the trace, shown in \Cref{lst:parmonad_example3}.  Each
trace entry is a tuple consisting of: the scheduling decision made,
the alternative decisions possible, and what the thread did.  As this
is right before deadlock, it is not surprising that there are no other
threads which could be scheduled.  There are only four calls to
\verb|getNumCapabilities| in the code we ported.  So we can look at
each, to find one where the surrounding context matches the trace.

\begin{listing}
\centering
\begin{cminted}{haskell}
go 0 _ | _IDLING_ON =
  do m <- newEmptyMVar
     r <- modifyHotVar idle (\is -> (m:is, is))
     numCapabilities <- getNumCapabilities
     if length r == numCapabilities - 1
       then do
         mapM_ (\vr -> putMVar vr True) r
       else do
         done <- takeMVar m
         if done
           then do
             return ()
           else do
             i <- getnext (-1::Int)
             go (maxtries numCapabilities) i
\end{cminted}
\caption[The source of the deadlock in the Par monad.]{The source of the deadlock in the Par monad.  In the ``then'' branch of the conditional, the \texttt{idle} list is not emptied when waking every blocked thread.}\label{lst:parmonad_example4}
\end{listing}

\Cref{lst:parmonad_example4} is our match.  When a thread is unable to
steal work, it creates an empty \verb|MVar| which it adds to a shared
list, called \verb|idle|; if that list already has an entry for every
other thread, they are woken by writing a value to their \verb|MVar|;
otherwise, the thread blocks by calling \verb|takeMVar| on its new,
empty, \verb|MVar|.

The final fragment of our execution trace corresponds to this code
where the condition \verb|length r == numCapabilities - 1| is false.
But how can that be false?  \emph{The list is never emptied!}  Think
about what happens if every thread reaches this logic \emph{twice}:

\begin{enumerate}
\item \verb|n - 1| threads add themselves to the list, and block.
\item The final thread adds itself to the list, wakes up the other
  threads.
\item \textbf{The list is not emptied, even though every thread is woken.}
\item \verb|n - 1| threads add themselves to the list, again, and
  block.
\item The final thread adds itself to the list.  It does \emph{not}
  trigger the wake-up logic, because the list is not the right length,
  and so it also blocks.
\item \textbf{Every thread is now blocked.}
\end{enumerate}

This problem is solved by writing \verb|[]| to \verb|idle| before
waking the threads.  The write must happen before, otherwise there is
a new race condition: one of the woken threads could add itself to the
list again, and then its \verb|MVar| be lost when the list is cleared.

\begin{listing}
  \begin{sublisting}{\textwidth}
    \centering
    \begin{cminted}{haskell}
makeScheds :: Int -> IO [Sched]
makeScheds main = do
   workpools <- replicateM numCapabilities $ R.newQ
   rngs <- replicateM numCapabilities $ Random.create >>= newHotVar
   idle <- newHotVar []
   sessionFinished <- newHotVar False
   sessionStacks   <- mapM newHotVar
     (replicate numCapabilities [Session baseSessionID sessionFinished])
   activeSessions  <- newHotVar S.empty
   sessionCounter  <- newHotVar (baseSessionID + 1)
   let allscheds = [ Sched { no=x, idle, isMain=(x==main), workpool=wp,
                             scheds=allscheds, rng=rng, sessions=stck
                             sessionCounter, activeSessions
                           }
                   | x   <- [0 .. numCapabilities-1]
                   | wp  <- workpools
                   | rng <- rngs
                   | stck <- sessionStacks
                   ]
   pure allscheds
    \end{cminted}
    \caption{Original}\label{lst:parmonad_orig}
  \end{sublisting}

  % [layout hack]: no gap between the listings otherwise
  \vspace{2.5em}

  \begin{sublisting}{\textwidth}
    \centering
    \begin{cminted}{haskell}
makeScheds :: (MonadConc m, MonadIO m) => Int -> m [Sched m]
makeScheds main = do
   numCapabilities <- getNumCapabilities
   workpools <- replicateM numCapabilities $ liftIO R.newQ
   let rng i = liftIO (Random.initialize (V.singleton $ fromIntegral i))
   rngs <- mapM (\i -> rng i >>= newHotVar) [0..numCapabilities]
   idle <- newHotVar []
   sessionFinished <- newHotVar False
   sessionStacks   <- mapM newHotVar
     (replicate numCapabilities [Session baseSessionID sessionFinished])
   activeSessions  <- newHotVar S.empty
   sessionCounter  <- newHotVar (baseSessionID + 1)
   let allscheds = [ Sched { no=x, idle, isMain=(x==main), workpool=wp,
                             scheds=allscheds, rng=rng, sessions=stck
                             sessionCounter, activeSessions
                           }
                   | x   <- [0 .. numCapabilities-1]
                   | wp  <- workpools
                   | rng <- rngs
                   | stck <- sessionStacks
                   ]
   pure allscheds
    \end{cminted}
    \caption{\dejafu{}}\label{lst:parmonad_dejafu}
  \end{sublisting}
  \caption{The Par monad ``direct'' scheduler initialisation.}\label{lst:parmonad}
\end{listing}

% [layout hack]: get lst:parmonad here.
\FloatBarrier

\subsection{Property-testing Concurrency}

The async\cite{async} library defines a type called
\verb|Concurrently|, a simple wrapper around \verb|IO|, which allows
concurrently composing actions with the \verb|Applicative| \verb|<*>|
operator.  \Cref{lst:concurrently} gives the implementation of
\verb|<*>| for \verb|Concurrently|.  Both the function and the
argument to apply it to are computed concurrently in separate threads,
and then combined.

\begin{listing}
\centering
\begin{cminted}{haskell}
newtype Concurrently a = Concurrently (IO a)

(<*>) :: Concurrently (a -> b) -> Concurrently a -> Concurrently b
(Concurrently f) <*> (Concurrently a) = Concurrently $ do
  fvar <- newEmptyMVar
  avar <- newEmptyMVar
  forkIO (f >>= putMVar fvar)
  forkIO (a >>= putMVar avar)
  f' <- readMVar fvar
  a' <- readMVar avar
  pure (f' a')
\end{cminted}
%$
\caption{Applicative concurrent composition for the \texttt{Concurrently} type.}\label{lst:concurrently}
\end{listing}

In addition to \verb|Applicative|, the \verb|Concurrently| type also
had a \verb|Monad| instance.  In Haskell, we like for our typeclasses
to have \emph{laws} specifying how instances should behave.  These
laws are not checked by the compiler: GHC is no theorem prover.
Rather, it is up to the author of a typeclass instance to ensure that
they follow all appropriate laws.  Failure by a library author to
follow the laws can lead to unexpected behaviour for users.  It should
come as no surprise that there is a law relating \verb|Applicative| to
\verb|Monad|: namely, that \verb|(<*>) = ap|.  The definition of
\verb|ap| is given in \Cref{lst:ap}.

\begin{listing}
\centering
\begin{cminted}{haskell}
ap :: Monad m => m (a -> b) -> m a -> m b
ap f a = do
  f' <- f
  a' <- a
  pure (f' a')
\end{cminted}
\caption{The \texttt{ap} function.}\label{lst:ap}
\end{listing}

Unfortunately, a user found that this \verb|ap| law did \emph{not}
hold for the \verb|Concurrently|
monad\footnote{\url{https://github.com/simonmar/async/pull/26}}.  With
the benefit of hindsight, the cause is clear: \verb|(<*>)| runs its
arguments concurrently, whereas \verb|ap| has no choice but to run its
arguments sequentially.  If the two arguments can concurrently
interfere with each other, then the \verb|(<*>)| case will exhibit
more nondeterminism than the \verb|ap| case.

Could \dejafu{} have helped here?  We believe so.
\Cref{lst:concurrently2} shows the \verb|Concurrently| type and its
\verb|(<*>)|, generalised to work for any \verb|MonadConc|.  The trick
here is to parameterise \verb|Concurrently| with the monad type it
will eventually operate in.  The only change to the body of
\verb|(<*>)| is renaming \verb|forkIO| to \verb|fork|.

\begin{listing}
\centering
\begin{cminted}{haskell}
newtype Concurrently m a = Concurrently (m a)

(<*>) :: MonadConc m => Concurrently m (a -> b) -> Concurrently m a -> Concurrently m b
(Concurrently f) <*> (Concurrently a) = Concurrently $ do
  fvar <- newEmptyMVar
  avar <- newEmptyMVar
  fork (f >>= putMVar fvar)
  fork (a >>= putMVar avar)
  f' <- readMVar fvar
  a' <- readMVar avar
  pure (f' a')
\end{cminted}
%$
\caption{The \texttt{Concurrently} type, generalised to any \texttt{MonadConc}.}\label{lst:concurrently2}
\end{listing}

We can now proceed to test the laws.  We used
QuickCheck\cite{claessen2000} for this, but any property-testing tool
which can generate functions and check monadic properties would do.
\Cref{lst:aplaw1} shows a \emph{passing} property: there is no
concurrent interference between the two arguments to \verb|(<*>)| and
\verb|ap|, and so the flaw does not manifest.

Until now, in the first two case studies, we have only seen \dejafu{}
used to generate visual traces.  The \verb|resultsSet| function is
another part of \dejafu{} which takes a \verb|MonadConc| action and
gives the set of its distinct results, with no traces.  So the
\verb|eq'| function in \Cref{lst:aplaw1} takes two \verb|ConcIO a|
values and checks that they produce the same sets of results.  The
\verb|ConcIO| type is an instance of \verb|MonadConc| which \dejafu{}
uses for testing purposes: it allows running a concurrent computation
and extracting a result in \verb|IO|.  For the reader unfamiliar with
\verb|QuickCheck|: \verb|Fun a b| represents a function of type
\verb|a -> b|; and \verb|monadicIO| converts an \verb|IO| action into
a \verb|Property| value, which passes if every call to \verb|assert|
inside it passes.

\begin{listing}
\centering
\begin{cminted}{haskell}
prop_monad_ap1 :: Ord b => Fun a b -> a -> Property
prop_monad_ap1 (apply -> f) a = (pure f <*> pure a) `eq` (pure f `ap` pure a)

eq :: Ord a => Concurrently ConcIO a -> Concurrently ConcIO a -> Property
eq (Concurrently left) (Concurrently right) = eq' left right

eq' :: Ord a => ConcIO a -> ConcIO a -> Property
eq' left right = monadicIO $ do
  l <- resultsSet defaultWay defaultMemType left
  r <- resultsSet defaultWay defaultMemType right
  assert (l == r)
\end{cminted}
%$
\caption{The \texttt{(<*>)} / \texttt{ap} law, with no concurrency.}\label{lst:aplaw1}
\end{listing}

Our \verb|prop_monad_ap1| property is uninteresting in a sense because
it is clearly free from concurrency errors: the very errors which we
want to detect!  It is free of them because the arguments to
\verb|(<*>)| and \verb|ap| are pure values, so there can be no
concurrent interference between them.  In order to observe the law
breaking, we must create a race condition.

\begin{listing}
\centering
\begin{cminted}{haskell}
prop_monad_ap2 :: Ord b => Fun a b -> Fun a b -> a -> Bool
prop_monad_ap2 (apply -> f) (apply -> g) a = go (<*>) `eq'` go ap where
  go combine = do
    flagvar <- newEmptyMVar
    let cf = do { flag <- tryPutMVar flagvar (); pure (if flag then f else g) }
    let ca = do { tryPutMVar flagvar (); pure a }
    let (Concurrently result) = Concurrently cf `combine` Concurrently ca
    pure result
\end{cminted}
\caption{The \texttt{(<*>)} / \texttt{ap} law, with concurrency.}\label{lst:aplaw2}
\end{listing}

\Cref{lst:aplaw2} contains such a race condition.  Rather than
generate one function and one value with QuickCheck, we instead
generate two functions and one value.  Then, when executing the
concurrent action, we use an \verb|MVar| to decide whether to apply
the first function or the second.  If the combining function,
\verb|(<*>)| or \verb|ap|, executes its arguments concurrently then we
would expect to see both functions tried; if it executes its arguments
sequentially, we will only see the results of the first function.

\begin{listing}
\begin{sublisting}{\textwidth}
\centering
\begin{cminted}{text}
> quickCheck (prop_monad_ap2 :: Fun Int String -> Fun Int String -> Int -> Property)
*** Failed! Falsifiable (after 3 tests and 8 shrinks):
{_->""}
{_->"a"}
0
\end{cminted}
\caption{The QuickCheck output.}\label{lst:aplaw3_quickcheck}
\end{sublisting}

% [layout hack]: no gap between the listings otherwise
\vspace{2.5em}

\begin{sublisting}{\textwidth}
\centering
\begin{cminted}{text}
> resultSet defaultWay defaultMemType (go (<*>) (\_ -> "") (\_ -> "a") 0)
fromList [Right "",Right "a"]

> resultSet defaultWay defaultMemType (go ap (\_ -> "") (\_ -> "a") 0)
fromList [Right "a"]
\end{cminted}
\caption{The \dejafu{} output.}\label{lst:aplaw3_dejafu}
\end{sublisting}
\caption{The result of the failing \texttt{(<*>)} / \texttt{ap} property.}\label{lst:aplaw3}
\end{listing}

Indeed, we do see the bug.  \Cref{lst:aplaw3} gives the QuickCheck and
\dejafu{} outputs.

We have the benefit of hindsight here, leading us towards the correct
test case to write in order to exhibit the bug.  We must ask, is it
unrealistic to expect a user to have the foresight to write a test
like this when they are first writing their \verb|Monad| instance?  We
think not.  When implementing functions for combining concurrent
monadic actions, it is no great leap to wonder what happens if there
are races between these actions.  The property may appear contrived,
but it is a natural way to investigate the effect of a race condition
in the \verb|(<*>)| / \verb|ap| law.

\section{Evaluation}
\label{sec:dejafu-evaluation}

A tool is effectively useless if it is too difficult to use.  The main
obstacle to the use of \dejafu{} is existing libraries which use
\verb|IO|; a programmer cannot simply use \verb|liftIO| everywhere,
without sacrificing completeness in all but trivial examples.
Ideally, existing libraries would be modified to support the
\verb|MonadConc| abstraction.  However, this does not seem a likely
short-term goal, and so a more promising way to approach the problem
is to provide alternatives to existing libraries.  As adapting code to
\verb|MonadConc| is often straightforward, as seen in the Par monad
case study, this is a viable short-term solution.

A second, more tractable, problem is integration with existing testing
frameworks; using \dejafu{} for little stand-alone programs is all
well and good, but to take off, it must be easy to use in the testing
context people are already familiar with.

\paragraph{Users}
Although \dejafu{} is a small one-man project, it does have some users
and contributors.  Seven users have opened issues on the GitHub issue
tracker\footnote{\url{https://github.com/barrucadu/dejafu/issues}}; a
further three have asked me for help over IRC and email; and five have
made small
contributions\footnote{\url{https://github.com/barrucadu/dejafu/graphs/contributors}}.
Two features came directly from user requests, motivated by
performance concerns in large test cases: these are (1) random
scheduling, which we discuss further in \Cref{chp:algorithms}; and (2)
the ability to discard uninteresting results or execution traces as
they are discovered, before evaluating the predicate at the end.  This
second point can have a significant impact: if all you are interested
in knowing is if your test case exhibits a particular failure, there
is no point in recording all the distinct results and then checking if
any of those exhibit the failure.  Much better to discard those
results which \emph{do not} exhibit the failure as they are
discovered, rather than keep them around in memory until the end.
\Cref{fig:discard} shows heap profiles of a simple test of \verb|MVar|
contention.

\begin{figure}
  \centering
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/nodiscard.png}
    \caption{Keeping all results and traces in memory.}
  \end{subfigure}
  \begin{subfigure}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{fig/discard.png}
    \caption{Discarding all traces, just keeping results.}
  \end{subfigure}
  \caption[Heap profiles of a test case for \texttt{MVar} contention.]{Heap profiles of a test case for \texttt{MVar} contention.  These plots are intended to be viewed with colour.}\label{fig:discard}
\end{figure}

Tweag I/O\footnote{\url{http://www.tweag.io/}}, a research and
development company based in Paris, are using \dejafu{} as part of
their work on the SAGE
project\footnote{\url{http://www.sagestorage.eu/}} on developing
storage systems for future supercomputers.  They cannot share specific
details of their work for commercial reasons, but one developer had
this to say (emphasis mine):

\begin{displayquote}
  Regarding the test case: we have an implementation of a distributed
  storage cluster, with possibly many nodes and parallel requests.
  The storage system itself is composed of several layers, which can
  be stacked on top of one another.  There is a lot of asynchronicity
  involved.  As per the tests themselves, I am testing parallel
  requests over different objects, etc.

  \emph{I'd like to add that dejafu tests are by far the most reliable
    tests in our suite, in my experience - I am yet to see a
    concurrency bug that they didn't spot, while some other tests
    missed them!}\cite{tweag2017}
\end{displayquote}

\subsection{Richness of the Abstraction}

As we noted in \Cref{sec:dejafu-scope}, there are some areas which we
do not currently aim to support with \dejafu{}:

\begin{itemize}
\item Blocking a thread until a file descriptor becomes available, as
  this introduces an additional source of nondeterminism.
\item Throwing an exception to a thread if it becomes deadlocked, as
  we cannot reliably detect deadlock involving only a subset of
  threads without support from the garbage collector.
\item Querying which capability (OS thread) a Haskell thread is
  running on, as this introduces an additional source of
  nondeterminism.
\end{itemize}

\dejafu{} does not aim to support the above three areas because we
believe that the desire for this functionality does not outweigh the
implementation cost.  If that belief changes, we will look for a way.

Introducing additional sources of nondeterminism into an SCT model is
difficult.  Simply introducing additional threads to model the
nondeterminism can cause an explosion of schedules tested, which is
unsatisfactory.  In this way, \verb|MonadConc| will always be limited
to what \dejafu{} can reasonably support.

However we can still push back the boundaries of what \dejafu{} supports.
Bound threads, Haskell threads which always run on the same unique OS
thread, were once out of scope as well.  This made it impossible to
use some C libraries with \verb|MonadConc|.  Now we have a prototype
implementation\footnote{\url{https://github.com/barrucadu/dejafu/issues/126}}
which shows promise, and which we intend to release.

\subsection{Writing and Porting Class-polymorphic Code}

\begin{listing}
\centering
\begin{cminted}{haskell}
instance MonadLogger (LoggerT STM) where -- ...
instance MonadLogger (LoggerT IO) where -- ...
\end{cminted}
\caption{Concrete instances for a typeclass-based logging abstraction.}\label{lst:mlogger1}
\end{listing}

We saw in the Par monad example that porting complex code to the
\verb|MonadConc| abstraction is not necessarily a difficult process.
However, this is not always the case.  Recently a user tried to port a
logging abstraction they made to \verb|MonadConc|.  Expressing this
with \verb|MonadConc| and \verb|MonadSTM| is not straightforward, as
constraints do not factor into instance selection.  So the instances
in \Cref{lst:mlogger2} overlap, and are illegal in standard Haskell.

\begin{listing}
\centering
\begin{cminted}{haskell}
instance MonadSTM  m => MonadLogger (LoggerT m) where -- ...
instance MonadConc m => MonadLogger (LoggerT m) where -- ...
\end{cminted}
\caption{Overlapping instances for a typeclass-based logging abstraction.}\label{lst:mlogger2}
\end{listing}

After some work, we introduced new \verb|IsSTM| and \verb|IsConc|
types to disambiguate between the two cases, and ended up with
\Cref{lst:mlogger3}.  The amount of effort required to arrive at this
solution led to the user questioning whether classes like their
\verb|MonadLogger| were even a useful abstraction at
all\footnote{\url{https://www.reddit.com/r/haskell/comments/7b1fbk/do_mtlstyle_effect_classes_really_pull_their/}}!
This is a good topic to think about, but it should not be prompted by
the effort of trying to use \dejafu{}.

\begin{listing}
\centering
\begin{cminted}{haskell}
instance MonadSTM  m => MonadLogger (LoggerT (IsSTM  m)) where -- ...
instance MonadConc m => MonadLogger (LoggerT (IsConc m)) where -- ...
\end{cminted}
\caption{Polymorphic instances for a typeclass-based logging abstraction.}\label{lst:mlogger3}
\end{listing}

So while porting code to the \verb|MonadConc| typeclass is often
simple when dealing with datatype and function definitions, it can be
more complicated when dealing with typeclasses.  It is not clear what
can be done to improve this matter.

\subsection{Library Alternatives}

There are popular Haskell libraries specifically for concurrency.  One
of these is the async\cite{async} library, for expressing asynchronous
computations.  This library is intended to be a higher-level and safer
way of expressing asynchronous computations with guaranteed cleanup
than using threading, mutable state, and asynchronous exceptions
directly.

The concurrency package, which provides \verb|MonadConc|, includes an
alternative to async using \verb|MonadConc|.  There is a test suite
using \dejafu{}.  The test suite for async itself just runs most tests
a single time, although one of them is run 1000 times.  Using
\dejafu{} here to automatically seek out interesting schedules is a
much more principled approach then repetition and hope.  Not all
features of async are supported, however.  As we do not currently
support bound threads, functions that use them have been omitted.

This is just one library, and providing an alternative library people
will have to switch to is far from optimal.  However, until library
authors start to use \dejafu{} and \verb|MonadConc| directly, such
alternatives will be needed to answer the question `Why should I use
this if I can't use it with anything I use already?'

\subsection{Tool Integration}

There are two popular libraries for unit testing in Haskell,
HUnit\cite{hunit} and Tasty\cite{tasty}.  From the perspective of the
user, the libraries are similar, but from the perspective of the
implementer, they have different approaches to integration.  The
hunit-dejafu\cite{hunit_dejafu} and tasty-dejafu\cite{tasty_dejafu}
packages provide integration with both.

These packages provide a common set of testing functions, an analogue
of Test.DejaFu but constructing values representing individual tests
which the frameworks can run, rather than executing and printing
results directly.  The test-framework\cite{test_framework} library is
also in common use, however it supports integration with HUnit, and so
needs no special support of its own.

The Tasty library is more featureful than HUnit, supporting progress
reporting, giving a message on success as well as failure, and
command-line arguments.  The tasty-dejafu package is similar to the
hunit-dejafu package and does not make use of these additional
features.

% \section{Conclusions}
% \label{sec:dejafu-conclusions}

% Although a commonly reported experience amongst Haskell programmers is
% that `if it compiles, it works,' there are times where it does
% \emph{not} work.  Concurrency is a particularly difficult area to get
% right, as everyone who has had to move outside the realm of guaranteed
% determinism will know.  A number of profiling and debugging tools
% exist, typically requiring special runtime support.  Yet there are no
% debugging tools for concurrent Haskell programs.  We contribute such a
% tool, at the cost of a programmer having to use a generalisation of
% the familiar concurrency abstraction.

% This work was inspired by attending a talk at INVEST 2014 by Paul
% Thompson, then a Ph.D student at Imperial College London, on
% systematic concurrency testing, the talk mentioned tools for languages
% like Java and C, but functional languages were not mentioned at all.
% The questions to be answered, then, were:

% \begin{itemize}
% \item \textbf{Can concurrency testing techniques from the imperative
%     and object-oriented worlds be applied in the functional world?}
%   \hfill

%   The answer to this would seem to be a resounding `yes.'  There was
%   some difficulty in implementing these techniques in a purely
%   functional setting, as the algorithms are typically expressed in
%   terms of mutable state, but this was overcome.

% \item \textbf{Does the purely functional setting allow for new
%     techniques to be developed?} \hfill

%   At first we hoped that the lack of effects in evaluation, amongst
%   other things, would allow for new techniques to be developed.
%   However, as concurrency testing explicitly cares about
%   \emph{execution} rather than \emph{evaluation} (although these are
%   one and the same in most languages), this does not seem to be the
%   case.
% \end{itemize}

% In \cite{walker2015}, we asked if the cost of a programmer needing to
% write their code in terms of \verb|MonadConc| rather than \verb|IO|
% was too high, and would discourage people.  This is still a concern,
% but with the development of libraries to integrate with or replace
% others, we hope that the use of \dejafu{} will appear attractive
% enough to overcome this.

% The contributions of this work are:

% \begin{itemize}
% \item a generalisation of a large subset of the GHC concurrency
%   abstraction;

% \item a library called \dejafu{} for the systematic and random testing
%   of concurrent Haskell programs, including those using relaxed-memory
%   effects; and

% \item a convincing demonstration that systematic concurrency
%   techniques work well in a rich concurrency setting such as Haskell.
% \end{itemize}

\section{Summary}

In this chapter we introduced \dejafu{}, our tool for testing
concurrent Haskell programs.  We use a typeclass abstraction over
concurrency, so we can swap out \verb|IO| and provide our own
implementation.  Our concurrency model includes most of the
Control.Concurrent\cite{control_concurrent} library module, although a
few operations are out of scope, or have their behaviour modified, as
discussed in \Cref{sec:dejafu-scope}.

\dejafu{} supports bounded partial-order reduction\cite{coons2013}
with relaxed memory\cite{zhang2015} as its core testing algorithm, but
also supports controlled random scheduling, implementing an algorithm
we discuss in \Cref{chp:algorithms}.  We have not attempted a formal
proof of correctness of \dejafu{}, but have a comprehensive test suite
and real-world users, suggesting that it works well in practice.

We have discussed three case studies, all of which involved applying
\dejafu{} to pre-existing code.  Such code must be modified to use the
\dejafu{} typeclass abstraction, but we have found this to be a
straightforward and type-directed process in most cases.  Finally, we
have evaluated \dejafu{} by discussing: the richness of its supported
concurrency abstraction; one user's experience of writing some
typeclass-polymorphic code; alternatives to existing concurrency
libraries, supporting the \dejafu{} abstractions; and the integration
of \dejafu{} with two popular Haskell testing tools.

\paragraph{Context}
\dejafu{} does not stand alone, it is related to our other
contributions:

\begin{itemize}
\item \Cref{chp:algorithms} discusses a new random scheduling
  algorithm for incomplete concurrency testing.  The chapter does not
  directly build on \dejafu{}, but \dejafu{} implements the algorithm.
\item \Cref{chp:coco} discusses a new property-discovery tool for
  functions operating on shared concurrent state.  The tool directly
  builds on \dejafu{} in two ways: (1) to discover these properties,
  and (2) by providing an interface for \dejafu{} to check them.
\end{itemize}
