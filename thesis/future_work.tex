We have made our contributions, and even found some users.  But
\dejafu{}, and concurrency testing in general, is not done yet.  We
now discuss future directions for the tools we have developed, and end
with a hopeful vision for concurrency testing in the future.

\paragraph{Measuring the quality of test suites}
How do we come to believe that a test suite is strong evidence for the
correctness of some program?  Any testing regimen is only as good as
its tests.  For sequential programs, we can use the traditional metric
of code coverage.  Code which is not covered at all usually has more
bugs than code which is covered by even low-quality
tests \parencite{ahmed2016}.  For concurrent programs, what metric do we
use?  If it is some notion of coverage, what is the space being
covered?  Here are two candidates:

\begin{itemize}
\item \emph{Schedule-sensitive branches} are often unintentional and
  erroneous points of synchronisation between concurrent
  threads \parencite{huang2015ssb}.  A good concurrency test suite should
  try all cases in a schedule-sensitive branch.

\item \emph{Unguarded shared state} without a synchronisation
  mechanism can lead to invalid or corrupt data.  If we have functions
  which operate on some mutable state of the same type, then a good
  concurrency test suite should check what happens when that state is
  shared and the functions are executed concurrently.
\end{itemize}

Both statement coverage and mutation score have only a weak negative
correlation with bug fixes \parencite{ahmed2016}, but there \emph{is} a
statistically significant difference between uncovered code and code
with some, even if low, coverage \parencite{ahmed2016}.  Being able to
identify the uncovered gaps of a concurrency test suite could greatly
help with improving the overall quality of a piece of software.

\paragraph{Maximal causality reduction for \dejafu{}}
The MCR algorithm \parencite{huang2015} explores a provably minimal number
of schedules required for completeness.  Typically this is orders of
magnitude fewer than the number of schedules constrained only by
dynamic partial-order reduction.  However, MCR is tricky to implement
in Haskell as it requires local determinism: the future actions of a
thread are determined solely by the prior actions of the same thread
and shared variables it has read.  Haskell breaks local determinism
with asynchronous exceptions, where one thread can kill another.

It may be possible to implement a Haskell-MCR by translating Haskell
execution traces into a simpler form suitable for MCR\@.  For example,
asynchronous exceptions can be modelled by giving each thread an
exception variable: throwing an exception to a thread writes to its
exception variable, and the thread checks its exception variable
before each action.  This polling technique is similar to how an
operating system can abstract over hardware interrupts: when an
interrupt arrives, its exception handler sets a flag and returns
control to the interrupted routine, which checks the flag at a
convenient point.

\paragraph{Optimal DPOR for \dejafu{}}
The Optimal DPOR with Observers algorithm \parencite{aronis2018} is
based on the same insight as MCR, that the order of writes to some
shared state only matters if there exists a read operation which
observes the difference.  As it is a DPOR algorithm, it could be
closer to what \dejafu{} currently does, and simpler to implement.
However, it is even more restrictive than MCR's requirement of local
determinism, forbidding threads from disabling each other entirely.
This means that blocking operations currently cannot be modelled with
the Optimal DPOR with Observers algorithm, so it is currently
unsuitable for \dejafu{}.

\paragraph{Accurately modelling delays in \dejafu{}}
Some users have expressed interest in using \dejafu{} to test systems
where accurate timing is
important\footnote{\url{https://github.com/barrucadu/dejafu/issues/130}},
such as distributed systems with timeouts.  However, \dejafu{}
currently has no notion of time.  A thread delaying is treated just
the same as a thread yielding.  It has no further effect on how
threads are scheduled during testing.

\begin{listing}
\centering
\begin{cminted}{haskell}
example :: MonadConc m => m Bool
example = do
    r <- newIORef False
    fork (threadDelay 1000000 >> writeIORef r True)
    readIORef r
\end{cminted}
\caption{A program with a large delay.}\label{lst:unreasonable}
\end{listing}

\Cref{lst:unreasonable} shows an example of a program with a delay: an
\verb|IORef| is created holding the value \verb|False|, which is set to
\verb|True| by another thread after a one hour delay.  Immediately
after forking the second thread, the \verb|IORef| is read and its value
returned.  What should a timing-aware \dejafu{} say about this
program?  Currently, both \verb|True| and \verb|False| are reported as
possible outcomes.  In reality, however, getting \verb|True| requires
the main thread to not be scheduled for the entire one hour delay,
which is vanishingly unlikely.  So should the \verb|True| case be
forbidden?  When there are multiple threads with delays which are
important relative to each other, the problem only becomes more
confusing.

\paragraph{Invariant testing in \dejafu{}}
Temporal program logics \parencite{pnueli1977} allow defining and
checking complex specifications for how a program should behave over
the course of its entire execution.  While \dejafu{} is not a temporal
logic checking tool, it can get part of the way there by allowing
users to specify invariants which are checked continuously while the
program under test runs.  As \dejafu{} drives the execution, these
invariants can be checked atomically.  This would be similar to, but
more general than, the GHC Haskell function
\verb|always :: STM Bool -> STM ()|, which registers an invariant to
be checked at the end of STM transactions.

\paragraph{Automatic interference for CoCo}
CoCo requires the user to provide the interference functions which are
necessary to distinguish between atomic and non-atomic actions.  An
alternative would be for CoCo to generate interference automatically.
This is possible, as the primitives (\verb|IORef|s, \verb|MVar|s, and
\verb|TVar|s) are all modelled by \dejafu{}.  However, it is difficult
to see how state with associated invariants can be automatically
modified.  The user would need to supply predicates to check these
invariants, or perhaps a collection of allowable transformations, but
is that any better than supplying the interference function directly?

\paragraph{Conditional properties in CoCo}
Speculate \parencite{braquehais2017} discovers conditional equations
and inequalities automatically, which greatly expands the range of
properties which can be found.  Conditional properties are useful as
we see how our functions behave in different situations, rather than
just in general.  CoCo has a limited form of conditional properties,
involving preconditions on the generated seed values.  Such
precondition functions must be supplied by the user.  However, it
would be much more useful if CoCo could synthesise preconditions, as
Speculate does, to discover interesting cases itself.

\paragraph{Term rewriting for CoCo}
Both QuickSpec \parencite{smallbone2017} and Speculate \parencite{braquehais2017}
use term rewriting to prune the discovered properties and to avoid
testing many cases.  Pruning by reducing to a normal form is difficult
to do with concurrency, as effects may be non-local.  For example,
consider with relaxed memory where writes to shared variables may be
delayed \parencite{zhang2015}.  Such behaviours make the effect of composing
two terms far less predictable.  Even so, it may still be possible in
some cases to use something like term rewriting to prune properties.

\paragraph{The future}
Testing a concurrent program goes something like this: (1)~write a
small concurrent program; (2)~run it lots of times with your
concurrency testing tool, recording the results of each execution;
(3)~look at the collection of results.  This approach is rather
different to how we test sequential programs.

Test cases for sequential programs are generally written in a
three-part ``given, when, then'' style \parencite{fowler2013}.  The
``given'' sets up the system under test, the ``when'' exercises it in
some way, and the ``then'' decides if the test passes or fails.  But
in a test case for a concurrent program, we may never reach the
``then''.  If a test case has some concurrency failure, such as
deadlock, we do not see the full picture.  Did the program only
deadlock, or did it also corrupt its state?  There is a fundamental
difference between normal program errors and concurrency errors.

Things do not need to be this way.  By controlling the concurrency, a
testing tool can ensure that even if the ``when'' component enters a
failure state, the ``then'' component is still executed.  When a
failure is reported, a \emph{simplified} description of the execution
of the ``when'' component, containing just the key details, can be
given to the user.  Maybe this is not a complete trace: the
\textsc{Concurrit} \parencite{elmas2013} tool offers an alternative
approach, where executions are represented by a small collection of
scheduling constraints.

However, testing concurrent programs will always remain a little more
difficult than testing sequential programs.  Concurrency bugs are
fundamentally more complex than other bugs, and there is only so much
that tooling and abstraction can accomplish.
