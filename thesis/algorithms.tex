The complete-within-a-bound approach of DPOR may not be feasible in large
programs.  We can sacrifice completeness, and instead explore the space of
schedules \emph{randomly}.  We may not find all bugs.  However we still want to
find \emph{most of them}.  Benchmarks show that some scheduling algorithms tend
to be better at this than others; not all algorithms are created equal.  In this
chapter we discuss a number of scheduling algorithms~\sref{algorithms-usual},
propose a new one based on a \emph{weighted} random selection of
threads~\sref{algorithms-swarm}, and show that it performs favourably in a
comparison of standard benchmark programs~\sref{algorithms-sctbench}.

\section{Concurrency Testing with Randomised Scheduling}
\label{sec:algorithms-usual}

Concurrency testing using randomised scheduling works by repeatedly executing a
concurrent program, exploring a particular schedule on each execution.  Unlike
systematic concurrency testing, little effort is made to avoid repetition of
schedules tested, so these algorithms are incomplete in general.  In this
section we present two approaches to randomised scheduling.

\paragraph{Controlled random scheduling}
A controlled random scheduler uses a random-number generator to choose threads
to execute. At each scheduling point, a runnable thread is randomly chosen using
a uniform distribution. This thread is then executed until the next scheduling
point. Like any controlled scheduling technique, the executed schedule can be
recorded and replayed. Additionally, a random scheduler can be used on programs
that exhibit nondeterminism beyond scheduler nondeterminism, although in this
case schedule replay will be unreliable\cite{thomson2016}.

\paragraph{Probabilistic concurrency testing}
The PCT algorithm\cite{burckhardt2010} uses a priority-based scheduler where the
highest priority runnable thread is chosen at each scheduling point. A bounded
number of \emph{priority change points} are inserted in the execution which
change the priority of the currently executing thread to a low value. These
change points are distributed uniformly over the length of the execution.

The algorithm is described as follows\cite{burckhardt2010}: given a program with
at most $n$ threads and at most $k$ steps, choose a bound $d$, then:

\begin{enumerate}
\item Randomly assign each of the $n$ threads a distinct initial priority value
from $\{d, d + 1, \ldots, d+n\}$. The lower priority values $\{1, \ldots, d−1\}$
are reserved for change points.
\item Uniformly pick integers $c_1, \ldots, c_{d−1}$ from $\{1, \ldots,
k\}$. These will be the priority change points.
\item Schedule threads strictly according to their priorities: never
schedule a thread if a higher priority thread is runnable. After executing the
$c_i$-th step $(1 \leq i < d)$, change the priority of the thread that executed
the step to $i$.
\end{enumerate}

PCT also introduces the idea of a ``bug depth''. The bug depth is defined as the
minimum set of ordering constraints between actions from different threads that
are sufficient to trigger the bug\cite{burckhardt2010}. Assuming a bug with
depth $d$, the probability of the PCT algorithm detecting the bug on a single
execution is $1/nk^{d−1}$.

The intuition behind PCT is that many concurrency bugs typically require
orderings between only a few actions in order to appear.

\section{Weighted Random Scheduling and Swarm Testing}
\label{sec:algorithms-swarm}

We now present \emph{swarm scheduling}, our new algorithm for finding
concurrency bugs through controlled scheduling.  The algorithm is inspired
by \emph{swarm testing}\cite{groce2012}, an approach to finding bugs using fuzz
testing more effectively.  Swarm testing makes the observation that, in a fuzz
tester with many available choices, the uniform selection of these is unlikely
to discover bugs which require a very unfair distribution to find:

\begin{displayquote}
  As a simple example, consider testing an implementation of a stack ADT that
  provides two operations, push and pop. [\ldots] To make this example more
  interesting, imagine the stack implementation has a capacity bug, and will
  crash whenever the stack is required to hold more than 32
  items.\cite{groce2012}
\end{displayquote}

The author then argues that tests generated by uniformly interleaving push and
pop operations is unlikely to produce a stack with more than 32 items, as items
would tend to be popped as quickly as they are pushed.  The proposed alternative
is, rather than having a \emph{single} optimal distribution for all tests,
generate \emph{multiple} distributions to encourage greater variety.

We transfer this idea to the context of scheduling algorithms by observing that
controlled random scheduling is much like the ``single optimal distribution''
for generating fuzz tests.  There are a number of options, and we simply pick
one according to a single pre-determined distribution.  So instead, we assign
a \emph{uniformly-chosen weight} to each new thread as it is created, and
schedule threads with a weighted random selection.  This approach is similar to
PCT, but less deterministic: PCT will always schedule the highest-weighted
runnable thread, whereas our approach is most likely to, but may not.  We can
also introduce weight change points, as in PCT, where we simply assign the
running thread a new weight uniformly.

With weight change points included, the algorithm is as follows: given a program
with at most $k$ steps, choose a range of weights $[w_{min}, w_{max}]$ and
a bound $d$, then:

\begin{enumerate}
\item Randomly assign the initial thread a weight from $[w_{min}, w_{max}]$.
\item Uniformly pick integers $c_1, \ldots, c_{d-1}$ from $\{1, \ldots, k\}$.
These will be the weight change points.
\item Schedule threads by a weighted random selection: at each scheduling point
use the weights of the enabled threads to construct a nonuniform distribution
and pick a thread to run until the next scheduling point.  As new threads are
created, randomly assign a weight from $[w_{min}, w_{max}]$.  After executing
the $c_i$-th step $(1 \leq i < d)$, change the weight of the thread that
executed the step to a random value from $[w_{min}, w_{max}]$.
\end{enumerate}

Multiple executions can use the same thread weights by recording the sequence of
generated weights in one execution, and just re-using this sequence in later
ones.  Unlike saving and re-using the random seed, this allows different
executions with the same weights to result in different scheduling decisions.
Weights can be re-used for a fixed number of executions by also recording how
many executions there have been, and throwing away the recorded values every $x$
executions, for some predetermined $x$.  \appref{swarm} contains our C++
implementation of swarm scheduling, including re-use of weights across
executions.  \dejafu{} has a Haskell implementation of swarm scheduling and of
controlled random scheduling, where the latter is implemented as a special case
of the former where all weights are equal.

\section{Benchmark Programs}
\label{sec:algorithms-sctbench}

\blindtext
